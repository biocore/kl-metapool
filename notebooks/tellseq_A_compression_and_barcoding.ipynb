{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:36:45.308869Z",
     "start_time": "2025-04-01T20:36:41.495239Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%reload_ext watermark\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import scipy.stats\n",
    "import yaml\n",
    "from metapool.metapool import *\n",
    "from metapool.util import (\n",
    "    join_dfs_from_files, extend_sample_accession_df,\n",
    "    extend_compression_layout_info, QIITA_STUDY_ID_KEY)\n",
    "from metapool.plate import PlateReplication, record_gdna_dilution\n",
    "from metapool import (add_controls, compress_plates, \n",
    "                      TUBECODE_KEY, SAMPLE_NAME_KEY, SAMPLE_DNA_CONC_KEY, \n",
    "                      NORMALIZED_DNA_VOL_KEY)\n",
    "from metapool.mp_strings import (\n",
    "    PM_SAMPLE_KEY, PM_WELL_KEY, PM_LIB_WELL_KEY, TELLSEQ_BARCODE_ID_KEY, \n",
    "    TELLSEQ_BARCODE_SET_ID_KEY)\n",
    "from metapool.util import warn_if_fp_exists\n",
    "%watermark -i -v -iv -m -h -p metapool,sample_sheet,openpyxl -u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:37:00.520039Z",
     "start_time": "2025-04-01T20:36:57.865407Z"
    }
   },
   "outputs": [],
   "source": [
    "! conda list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knight Lab TellSeq pipeline notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 (of 5): Workflow for normalizing DNA\n",
    "\n",
    "This portion of the notebook will read in the output of the mini-Pico quantification assay and construct an Echo normalization picklist file. \n",
    "\n",
    "As inputs, it requires:\n",
    "1. A tab-delimited row-wise sample accession file that indicates the sample name (`sample_name`) and its associated matrix tube barcode (`TubeCode`)\n",
    "2. A tab-delimited metadata file downloaded from Qiita\n",
    "3. An accurate plate compression form, with appropriate VisionMate barcode scanner files (`Plate map file`)\n",
    "4. One or more DNA concentration files: at least one for the undiluted plate, and as many additional ones for other dilutions as desired.\n",
    "5. If performing absolute quant, a DNA concentration file for the undiluted plate is required.  This may be the same as one of the other concentration files specified earlier.\n",
    "\n",
    "The workflow then:\n",
    "1. reads in the specified input files and constructs a dataframe\n",
    "2. calculates volumes to be added via echo to reach desired input DNA quantity, with info on which samples need to be pulled from a diluted plate and which from the original plate\n",
    "3. produces an Echo-formatted pick list file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 of 5, Step 0 of 8: Provide inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:37:03.384672Z",
     "start_time": "2025-04-01T20:37:03.380572Z"
    }
   },
   "outputs": [],
   "source": [
    "## INPUT\n",
    "expt_name = \"RKLtest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:37:04.061022Z",
     "start_time": "2025-04-01T20:37:04.057399Z"
    }
   },
   "outputs": [],
   "source": [
    "## INPUT\n",
    "# One dictionary per study included in the samples on this run.\n",
    "studies_info = [\n",
    "    # EVERY entry in the dictionary must be specifically updated \n",
    "    # *every* time this notebook is run--none of these have defaults!\n",
    "    {\n",
    "    'Project Name': 'TestProjA_10002', # PROJECTNAME_QIITAID\n",
    "    'Project Abbreviation': 'TestProjA', # PROJECTNAME\n",
    "    'sample_accession_fp': './test_data/Plate_Maps/Tellseq_TestProjA - 10002 - Sample Accession.csv',\n",
    "    'qiita_metadata_fp': './test_data/Plate_Maps/10002_20241004-110731.txt',\n",
    "    'experiment_design_description': 'plasma sequencing',\n",
    "    'HumanFiltering': 'True', \n",
    "    'Email': 'r@gmail.com'\n",
    "    }  \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:37:04.807114Z",
     "start_time": "2025-04-01T20:37:04.802728Z"
    }
   },
   "outputs": [],
   "source": [
    "## INPUT\n",
    "compression_layout = [\n",
    "    {\n",
    "        # top left plate\n",
    "        'Plate Position': 1, # as int\n",
    "        'Plate map file': './test_data/Plate_Maps/Tellseq_Test_Plate_1.tsv',\n",
    "        'Project Name': 'TestProjA_10002', # PROJECTNAME_QIITAID\n",
    "        'Project Plate': 'Plate_1', # Plate_#\n",
    "        'Plate elution volume': 70\n",
    "    },\n",
    "    {\n",
    "        # top right plate\n",
    "        'Plate Position': 2, # as int\n",
    "        'Plate map file': './test_data/Plate_Maps/Tellseq_Test_Plate_2.tsv',\n",
    "        'Project Name': 'TestProjA_10002', # PROJECTNAME_QIITAID\n",
    "        'Project Plate': 'Plate_2', # Plate_#\n",
    "        'Plate elution volume': 70\n",
    "    },\n",
    "    {\n",
    "        # bottom left plate\n",
    "        'Plate Position': 3, # as int\n",
    "        'Plate map file': './test_data/Plate_Maps/Tellseq_Test_Plate_3.tsv',\n",
    "        'Project Name': 'TestProjA_10002', # PROJECTNAME_QIITAID\n",
    "        'Project Plate': 'Plate_3', # Plate_#\n",
    "        'Plate elution volume': 70\n",
    "    },\n",
    "    {\n",
    "        # bottom right plate\n",
    "        'Plate Position': 4, # as int\n",
    "        'Plate map file': './test_data/Plate_Maps/Tellseq_Test_Plate_4.tsv',\n",
    "        'Project Name': 'TestProjA_10002', # PROJECTNAME_QIITAID \n",
    "        'Project Plate': 'Plate_4',  # Plate_#\n",
    "        'Plate elution volume': 70\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:37:05.505371Z",
     "start_time": "2025-04-01T20:37:05.501433Z"
    }
   },
   "outputs": [],
   "source": [
    "# CONSTANTS: Users, DO NOT CHANGE THESE\n",
    "# values without consulting with tech team\n",
    "\n",
    "# Mask arrays for even and odd rows and columns\n",
    "EVEN_ROWS = [x for x in range(16) if x % 2 == 0]\n",
    "ODD_ROWS = [x for x in range(16) if x % 2 == 1]\n",
    "EVEN_COLS = [x for x in range(24) if x % 2 == 0]\n",
    "ODD_COLS = [x for x in range(24) if x % 2 == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:37:06.374610Z",
     "start_time": "2025-04-01T20:37:06.370003Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_studies_attr_list(studies_dict, desired_key):\n",
    "    return [x[desired_key] for x in studies_dict]\n",
    "\n",
    "def pick_expected_separator(fps_list):\n",
    "    sep = \"\\t\"\n",
    "    visible_sep = \"tab\"\n",
    "    \n",
    "    num_fps = len(fps_list)\n",
    "    num_csv = sum([x.endswith('.csv') for x in fps_list])\n",
    "    num_txt = sum([x.endswith('.txt') for x in fps_list])\n",
    "    num_tsv = sum([x.endswith('.tsv') for x in fps_list])\n",
    "    \n",
    "    if num_csv == num_fps:\n",
    "        sep = ','\n",
    "        visible_sep = \"comma\"\n",
    "    elif (num_tsv + num_txt) != num_fps:\n",
    "        warnings.warn(\n",
    "            \"Could not determine separator; defaulting to \" + visible_sep)\n",
    "\n",
    "    return sep, visible_sep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 of 5, Step 1 of 8: Read in sample accession files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:37:07.720697Z",
     "start_time": "2025-04-01T20:37:07.717178Z"
    }
   },
   "outputs": [],
   "source": [
    "# read in the sample accession files\n",
    "sample_accession_fps = get_studies_attr_list(\n",
    "    studies_info, 'sample_accession_fp')\n",
    "sample_acc_sep, sa_sep_name = pick_expected_separator(sample_accession_fps)\n",
    "print(f\"Expected sample accession separator: {sa_sep_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:37:08.367010Z",
     "start_time": "2025-04-01T20:37:08.338971Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_accession_df = join_dfs_from_files(\n",
    "    sample_accession_fps, [SAMPLE_NAME_KEY, TUBECODE_KEY], sep=sample_acc_sep)\n",
    "sample_accession_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:37:09.034105Z",
     "start_time": "2025-04-01T20:37:09.026462Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_accession_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 of 5, Step 2 of 8: Read in the sample info from Qiita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:37:10.278555Z",
     "start_time": "2025-04-01T20:37:10.274967Z"
    }
   },
   "outputs": [],
   "source": [
    "# read in the qiita metadata files\n",
    "qiita_metadata_fps = get_studies_attr_list(studies_info, 'qiita_metadata_fp')\n",
    "qiita_metadata_sep, qm_sep_name = pick_expected_separator(qiita_metadata_fps)\n",
    "print(f\"Expected qiita metadata separator: {qm_sep_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:37:10.837194Z",
     "start_time": "2025-04-01T20:37:10.828493Z"
    }
   },
   "outputs": [],
   "source": [
    "metadata_df = join_dfs_from_files(\n",
    "    qiita_metadata_fps, [SAMPLE_NAME_KEY, QIITA_STUDY_ID_KEY], \n",
    "    opt_cols_to_extract=['tube_id'], unique_cols=[SAMPLE_NAME_KEY],\n",
    "    sep=qiita_metadata_sep)\n",
    "metadata_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:37:11.531039Z",
     "start_time": "2025-04-01T20:37:11.524736Z"
    }
   },
   "outputs": [],
   "source": [
    "metadata_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the metadata to link the study info into the sample accession dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:37:12.772684Z",
     "start_time": "2025-04-01T20:37:12.747551Z"
    }
   },
   "outputs": [],
   "source": [
    "extended_sample_accession_df = extend_sample_accession_df(\n",
    "    sample_accession_df, studies_info, metadata_df)\n",
    "extended_sample_accession_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 of 5, Step 3 of 8: Assign the compression layout and add controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:37:14.073280Z",
     "start_time": "2025-04-01T20:37:14.070542Z"
    }
   },
   "outputs": [],
   "source": [
    "## INPUT\n",
    "blanks_dir = './test_data/BLANKS_for_tellseq'\n",
    "\n",
    "## INPUT\n",
    "# ATTENTION: Does your plate include katharoseq controls?\n",
    "# If *yes*, replace the None below with the path to the directory they are in, such as\n",
    "# katharoseq_dir = './test_data/katharoseq'\n",
    "katharoseq_dir = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:37:14.714691Z",
     "start_time": "2025-04-01T20:37:14.711715Z"
    }
   },
   "outputs": [],
   "source": [
    "# copy study info into the compression layout dictionary (so that it doesn't \n",
    "# have to be entered manually in both places)\n",
    "extended_compression_layout = extend_compression_layout_info(\n",
    "    compression_layout, studies_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:37:15.737410Z",
     "start_time": "2025-04-01T20:37:15.572091Z"
    }
   },
   "outputs": [],
   "source": [
    "plate_df = compress_plates(extended_compression_layout, \n",
    "                           extended_sample_accession_df, well_col=PM_WELL_KEY)\n",
    "plate_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for samples with missing names; at this point, we expect all blanks\n",
    "and katharoseq controls WON'T have names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:37:17.227505Z",
     "start_time": "2025-04-01T20:37:17.223120Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_nan_samples(a_plate_df, a_blanks_dir=None):\n",
    "    num_remaining_nans = a_plate_df[a_plate_df[PM_SAMPLE_KEY].isna()].shape[0]\n",
    "    print(\"Number of samples with missing names: %d\" % num_remaining_nans)\n",
    "    \n",
    "    if num_remaining_nans > 0 and a_blanks_dir:\n",
    "        err_msg = f\"\"\"\n",
    "By now, all samples should have names, so **do not continue** before fixing this!\n",
    "\n",
    "\"Unofficial\" blanks are the most likely issue.\n",
    "Determine if the tube codes for the problem samples (shown below) are blanks.\n",
    "If they are, add them to the missing_blanks.csv file in the {a_blanks_dir} directory.\n",
    "Then re-run from 'Part 1 of 5, Step 3 of 8: Assign the compression layout and add controls'.\"\"\"\n",
    "        print(err_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:37:17.936907Z",
     "start_time": "2025-04-01T20:37:17.933267Z"
    }
   },
   "outputs": [],
   "source": [
    "check_nan_samples(plate_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:37:18.681173Z",
     "start_time": "2025-04-01T20:37:18.667649Z"
    }
   },
   "outputs": [],
   "source": [
    "plate_df = add_controls(plate_df, blanks_dir, katharoseq_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After adding controls, check again for samples with missing names; \n",
    "at this point, we expect all blanks and katharoseq controls WILL have names, \n",
    "so if there are any remaining samples without names, \n",
    "stop processing and fix them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:37:20.465697Z",
     "start_time": "2025-04-01T20:37:20.457213Z"
    }
   },
   "outputs": [],
   "source": [
    "## DECISION -- stop if there are still samples without names\n",
    "check_nan_samples(plate_df, a_blanks_dir=blanks_dir)\n",
    "plate_df[plate_df[PM_SAMPLE_KEY].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 of 5, Step 4 of 8: Validate plate dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:37:22.219591Z",
     "start_time": "2025-04-01T20:37:22.206171Z"
    }
   },
   "outputs": [],
   "source": [
    "# note that this function does not *need* the extended sample accession df,\n",
    "# but it is easier to use it just to keep things consistent\n",
    "validate_plate_df(plate_df,metadata_df, extended_sample_accession_df, \n",
    "                  blanks_dir, katharoseq_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 of 5, Step 5 of 8: read in DNA concentrations and add to plate map\n",
    "\n",
    "Enter the path to each of the Pico DNA concentration output files. Each one should be\n",
    " a tab-separated file produced by the MiniPico assay on the condensed, \n",
    " 384-well plate, and should have a format like the below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "##BLOCKS= 1\n",
    "Group: Unknowns\n",
    "Sample\tWells\tRFU_Values\tConcentration\tMean_Conc\tSD\tCV\tDilution\tAdjConc\t\n",
    "01\tA1\t528791.000\t2.472\t2.472\t0.000\t0.0\t\t\t\n",
    "02\tC1\t481728.000\t2.282\t2.282\t0.000\t0.0\t\t\t\n",
    "03\tE1\t462964.000\t2.206\t2.206\t0.000\t0.0\t\t\t\n",
    "04\tG1\t556609.000\t2.585\t2.585\t0.000\t0.0\t\t\t\n",
    "05\tI1\t710679.000\t3.207\t3.207\t0.000\t0.0\t\t\t\n",
    "06\tK1\t655693.000\t2.985\t2.985\t0.000\t0.0\t\t\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:37:24.489833Z",
     "start_time": "2025-04-01T20:37:24.487310Z"
    }
   },
   "outputs": [],
   "source": [
    "BASE_DILUTION_NAME = 'undiluted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:37:25.570010Z",
     "start_time": "2025-04-01T20:37:25.566771Z"
    }
   },
   "outputs": [],
   "source": [
    "## INPUT\n",
    "# Add one entry for each dilution, where the key is a descriptive name\n",
    "# for the dilution and the value is the path to the concentrations file.  \n",
    "# Order from most to least preferred for use.\n",
    "\n",
    "## IMPORTANT: The least preferred, fallback file must be the LAST ONE, \n",
    "# named with BASE_DILUTION_NAME\n",
    "\n",
    "dilutions_infos = {\n",
    "    \"10to1dilution\": './test_data/Quant/MiniPico/Tellseq_gDNA_diluted_10_to_1_Quant.txt',\n",
    "    \"2to1dilution\": './test_data/Quant/MiniPico/Tellseq_gDNA_diluted_2_to_1_Quant.txt',\n",
    "    BASE_DILUTION_NAME: './test_data/Quant/MiniPico/Tellseq_gDNA_Original_Quant.txt'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:37:26.522379Z",
     "start_time": "2025-04-01T20:37:26.519307Z"
    }
   },
   "outputs": [],
   "source": [
    "for curr_fp in dilutions_infos.values():\n",
    "    if not os.path.isfile(curr_fp):\n",
    "        print(\"Problem! %s is not a path to a valid file\" % curr_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:37:27.544119Z",
     "start_time": "2025-04-01T20:37:27.502767Z"
    }
   },
   "outputs": [],
   "source": [
    "plate_df = load_concentrations(plate_df, dilutions_infos, 'SpectraMax_i3x')\n",
    "plate_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:37:28.762762Z",
     "start_time": "2025-04-01T20:37:28.760168Z"
    }
   },
   "outputs": [],
   "source": [
    "## INPUT -- verify default\n",
    "# Add dilution info into the plate df; for every sample with a DILUTED \n",
    "# concentration greater than or equal to the min concentration threshold, we\n",
    "# SHOULD use the diluted plate values.\n",
    "min_conc_threshold = 1.5  # ng/ul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:37:29.562928Z",
     "start_time": "2025-04-01T20:37:29.559842Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_conc_col_name(dilution_name):\n",
    "    return f\"{SAMPLE_DNA_CONC_KEY}_{dilution_name}\"\n",
    "\n",
    "def get_gte_thresh_mask(a_plate_df, conc_key, threshold):\n",
    "    return a_plate_df[conc_key] >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:37:30.448044Z",
     "start_time": "2025-04-01T20:37:30.423868Z"
    }
   },
   "outputs": [],
   "source": [
    "plate_df = select_sample_dilutions(\n",
    "    plate_df, list(dilutions_infos.keys()), \n",
    "    lambda x, y: get_gte_thresh_mask(x, y, min_conc_threshold),\n",
    "    make_conc_col_name)\n",
    "plate_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize plate DNA concentrations and plate map:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:37:37.221396Z",
     "start_time": "2025-04-01T20:37:37.218510Z"
    }
   },
   "outputs": [],
   "source": [
    "preferred_dilution_name = list(dilutions_infos.keys())[0]\n",
    "base_conc_key = make_conc_col_name(BASE_DILUTION_NAME) \n",
    "diluted_conc_key = make_conc_col_name(preferred_dilution_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:37:38.182849Z",
     "start_time": "2025-04-01T20:37:38.003019Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.histplot(plate_df[base_conc_key],bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:37:40.959546Z",
     "start_time": "2025-04-01T20:37:40.956Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_dilution_regression(\n",
    "        a_plate_df, base_conc_name=base_conc_key, \n",
    "        diluted_name=diluted_conc_key):\n",
    "    res = scipy.stats.linregress(plate_df[base_conc_name],plate_df[diluted_name])\n",
    "    plt.title(f'Calculated dilution factor from regression: {res.slope:.3f}, \\n Person correlation: {res.rvalue:.3f}')  \n",
    "    sns.regplot(x=base_conc_name,y=diluted_name,data=a_plate_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:37:41.823535Z",
     "start_time": "2025-04-01T20:37:41.820284Z"
    }
   },
   "outputs": [],
   "source": [
    "dilution_factor_key = f\"dilution_factor_{preferred_dilution_name}\"\n",
    "plate_df[dilution_factor_key] = plate_df[diluted_conc_key]/plate_df[base_conc_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:37:44.723601Z",
     "start_time": "2025-04-01T20:37:44.720168Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_dilution_factor(\n",
    "        a_plate_df, a_dilution_conc_key, a_dilution_factor_key, use_log_y=True):\n",
    "    sns.scatterplot(\n",
    "        x=a_dilution_conc_key,y=a_dilution_factor_key,data=a_plate_df)\n",
    "    if use_log_y:\n",
    "        plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:37:46.138445Z",
     "start_time": "2025-04-01T20:37:46.004917Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_dilution_factor(plate_df, base_conc_key, dilution_factor_key, use_log_y=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:37:48.042484Z",
     "start_time": "2025-04-01T20:37:47.917873Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_dilution_factor(plate_df, diluted_conc_key, dilution_factor_key, use_log_y=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize plate DNA concentrations and plate map:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:37:52.511759Z",
     "start_time": "2025-04-01T20:37:52.506605Z"
    }
   },
   "outputs": [],
   "source": [
    "def display_concentration_heatmap(a_plate_df, conc_key, plate_num=None):\n",
    "    plate_index_sets = {\n",
    "        1: [EVEN_ROWS, EVEN_COLS],\n",
    "        2: [EVEN_ROWS, ODD_COLS],\n",
    "        3: [ODD_ROWS, EVEN_COLS],\n",
    "        4: [ODD_ROWS, ODD_COLS]\n",
    "    }\n",
    "    \n",
    "    dna_concs = make_2D_array(a_plate_df, data_col=conc_key, \n",
    "                              well_col=PM_WELL_KEY).astype(float)\n",
    "    \n",
    "    # get information for annotation\n",
    "    names = make_2D_array(\n",
    "        a_plate_df, data_col=PM_SAMPLE_KEY, well_col=PM_WELL_KEY)\n",
    "    \n",
    "    plot_concs = dna_concs\n",
    "    plot_names = names\n",
    "    if plate_num is not None:\n",
    "        plot_concs = dna_concs[np.ix_(*plate_index_sets[plate_num])]\n",
    "        plot_names = names[np.ix_(*plate_index_sets[plate_num])]\n",
    "    \n",
    "    plot_plate_vals(plot_concs,\n",
    "                    annot_str=plot_names,\n",
    "                    color_map='viridis',\n",
    "                    annot_fmt='.5s')\n",
    "    \n",
    "def display_concentration_heatmap_by_dilution(a_plate_df, dilution_name, plate_num=None):\n",
    "    plate_str = \"\" if plate_num is None else f\" plate {plate_num}\"\n",
    "    print(f\"Concentration heatmap for {dilution_name}{plate_str}\")\n",
    "    display_concentration_heatmap(\n",
    "        a_plate_df, make_conc_col_name(dilution_name), \n",
    "        plate_num=plate_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the 384-well visualization of the most preferred dilution plate and the base dilution plate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:37:56.369441Z",
     "start_time": "2025-04-01T20:37:54.234813Z"
    }
   },
   "outputs": [],
   "source": [
    "display_concentration_heatmap_by_dilution(plate_df, preferred_dilution_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:37:59.802720Z",
     "start_time": "2025-04-01T20:37:57.598396Z"
    }
   },
   "outputs": [],
   "source": [
    "display_concentration_heatmap_by_dilution(plate_df, BASE_DILUTION_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make sample replicates\n",
    "\n",
    "Set replicate dictionary, if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:38:14.769480Z",
     "start_time": "2025-04-01T20:38:14.766117Z"
    }
   },
   "outputs": [],
   "source": [
    "# Replicate formats:\n",
    "# replicate_dict = {source1_quadrant:destination1_quadrant}\n",
    "# replicate_dict = {source1_quadrant:[destination1_quadrants,destination1_quadrants]}\n",
    "# Replicate example: \n",
    "# replicate_dict = {1:[2,3]}\n",
    "# for no replicates, use:\n",
    "replicate_dict = None\n",
    "\n",
    "# 'Well' differs from 'Library Well' because the former specifies the \n",
    "# gDNA source well while the latter specifies the well (destination well) that \n",
    "# will contain the sequencing library for the sample. These contain the same\n",
    "# info when replicates are not used, but differ when replicates ARE used,\n",
    "# so it is safer to use 'Library Well' in both cases.\n",
    "# (Careful!  well_col is a global variable used throughout rest of notebook)\n",
    "well_col = PM_LIB_WELL_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:38:15.504045Z",
     "start_time": "2025-04-01T20:38:15.495775Z"
    }
   },
   "outputs": [],
   "source": [
    "# initialize new PlateReplication object to manage metadata, conversions, etc.\n",
    "# initialize w/preferred well_col.\n",
    "pr = PlateReplication(well_col)\n",
    "\n",
    "# set overwrite=False to detect any overwriting of source or destination quads \n",
    "# and raise an Error.\n",
    "plate_df = pr.make_replicates(\n",
    "    plate_df, replicates=replicate_dict, overwrite=True)\n",
    "\n",
    "# replicates overlapping sample_wells for other samples should raise warning,\n",
    "# but will be allowed\n",
    "if 'True' in plate_df['contains_replicates'].unique():\n",
    "    raise NotImplementedError(\"This notebook does not yet support replicates.\")\n",
    "    \n",
    "    # plate_df['contains_replicates'] = True\n",
    "    # # get DNA concentration information\n",
    "    # dna_concs = make_2D_array(plate_df, data_col='Sample DNA Concentration', \n",
    "    #                           well_col=well_col).astype(float)\n",
    "    # \n",
    "    # # get information for annotation\n",
    "    # names = make_2D_array(plate_df, data_col=PM_SAMPLE_KEY, well_col=well_col)\n",
    "    # \n",
    "    # plot_plate_vals(dna_concs,\n",
    "    #             annot_str=names,\n",
    "    #             color_map='viridis',\n",
    "    #             annot_fmt='.6s')\n",
    "else:\n",
    "    plate_df['contains_replicates'] = False\n",
    "    \n",
    "# show whether this plate contains replicates or not\n",
    "f\"Contains replicates: {plate_df['contains_replicates'].unique()}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gDNA concentration heatmap, Plate 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:38:18.379393Z",
     "start_time": "2025-04-01T20:38:17.268487Z"
    }
   },
   "outputs": [],
   "source": [
    "display_concentration_heatmap_by_dilution(plate_df, BASE_DILUTION_NAME, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gDNA concentration heatmap, Plate 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:38:22.458Z",
     "start_time": "2025-04-01T20:38:21.328334Z"
    }
   },
   "outputs": [],
   "source": [
    "display_concentration_heatmap_by_dilution(plate_df, BASE_DILUTION_NAME, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gDNA concentration heatmap, Plate 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:38:26.314475Z",
     "start_time": "2025-04-01T20:38:25.193146Z"
    }
   },
   "outputs": [],
   "source": [
    "display_concentration_heatmap_by_dilution(plate_df, BASE_DILUTION_NAME, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### gDNA concentration heatmap, Plate 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:38:29.864993Z",
     "start_time": "2025-04-01T20:38:28.654968Z"
    }
   },
   "outputs": [],
   "source": [
    "display_concentration_heatmap_by_dilution(plate_df, BASE_DILUTION_NAME, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 of 5, Step 6 of 8: calculate normalization volumes and add to plate map\n",
    "\n",
    "This step will calculate volumes for the DNA normalization pick list.\n",
    "\n",
    "Check the desired values for:\n",
    " - **`ng`**: the desired quantity of DNA in normed plate, in ng\n",
    " - **`total_vol`**: the total volume of normalized DNA, in nL\n",
    " - **`min_vol`**: the minimum quantity of sample to add, in nL\n",
    " - **`resolution`**: the resolution of the Echo, in nL (usually 2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:38:31.636561Z",
     "start_time": "2025-04-01T20:38:31.633486Z"
    }
   },
   "outputs": [],
   "source": [
    "## INPUT -- verify defaults\n",
    "ng = 7.5\n",
    "total_vol = 5000\n",
    "min_vol = 25\n",
    "resolution = 2.5"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "add_vols_in_nl_to_plate_df(\n",
    "    plate_df, target_mass_ng=ng, min_vol_in_nl=min_vol, \n",
    "    max_vol_in_nl=total_vol, instrument_resolution_in_nl=resolution)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "add_pool_input_dna_mass_ng_to_plate_df(plate_df)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "plate_df.head()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 of 5, Step 7 of 8 (optional): Add synDNA spike-in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:38:34.330553Z",
     "start_time": "2025-04-01T20:38:34.327254Z"
    }
   },
   "outputs": [],
   "source": [
    "## INPUT\n",
    "# Set syndna_pool_number to 1 if syndna is being used; otherwise, leave as None\n",
    "syndna_pool_number = None\n",
    "syndna_picklist_fp = './test_output/Input_Norm/Tellseq_matrix_syndna_absquant.txt'\n",
    "# The below fp can be the same as the concentration file input at step 1.5, \n",
    "# and generally will be EXCEPT if there was a dilution done between the \n",
    "# elution and the concentration measurement at step 1.5 (e.g., for NPH),\n",
    "undiluted_gdna_conc_fp = './test_data/Quant/MiniPico/Tellseq_gDNA_Original_Quant.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:38:35.340694Z",
     "start_time": "2025-04-01T20:38:35.336456Z"
    }
   },
   "outputs": [],
   "source": [
    "plate_df = add_syndna(plate_df, \n",
    "                      syndna_pool_number=syndna_pool_number,\n",
    "                      syndna_concentration=2.22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:38:37.366835Z",
     "start_time": "2025-04-01T20:38:37.362732Z"
    }
   },
   "outputs": [],
   "source": [
    "f'For this plate, is_absquant = {is_absquant(plate_df)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:38:38.769654Z",
     "start_time": "2025-04-01T20:38:38.765672Z"
    }
   },
   "outputs": [],
   "source": [
    "if is_absquant(plate_df):\n",
    "    # add undiluted gdna concentrations to plate_df\n",
    "    plate_df = add_undiluted_gdna_concs(plate_df, undiluted_gdna_conc_fp)     \n",
    "\n",
    "    # create syndna picklist    \n",
    "    syndna_well='A1'\n",
    "    syndna_plate = 'synDNA plate'\n",
    "    syndna_picklist = \\\n",
    "        format_dna_norm_picklist(\n",
    "            np.array(plate_df['synDNA volume']),\n",
    "            np.zeros(plate_df.shape[0]),\n",
    "            np.repeat(syndna_well,plate_df.shape[0]),\n",
    "            dest_wells = np.array(plate_df[well_col]),\n",
    "            sample_names = np.array(plate_df[PM_SAMPLE_KEY]),\n",
    "            sample_plates = np.repeat(syndna_plate,plate_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:38:39.555762Z",
     "start_time": "2025-04-01T20:38:39.552785Z"
    }
   },
   "outputs": [],
   "source": [
    "if is_absquant(plate_df):\n",
    "    if os.path.isfile(syndna_picklist_fp):\n",
    "        print(\"Warning! This file exists already.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:38:40.289244Z",
     "start_time": "2025-04-01T20:38:40.285869Z"
    }
   },
   "outputs": [],
   "source": [
    "if is_absquant(plate_df):\n",
    "    with open(syndna_picklist_fp, 'w') as f:\n",
    "        f.write(syndna_picklist)\n",
    "\n",
    "    !head {syndna_picklist_fp}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 of 5, Step 8 of 8: Make pick list and write to file\n",
    "\n",
    "Format the Echo-compatible pick list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:38:41.750625Z",
     "start_time": "2025-04-01T20:38:41.747971Z"
    }
   },
   "outputs": [],
   "source": [
    "## INPUT\n",
    "norm_picklist_fp = './test_output/Input_Norm/Tellseq_inputnorm.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:38:42.534992Z",
     "start_time": "2025-04-01T20:38:42.528123Z"
    }
   },
   "outputs": [],
   "source": [
    "norm_picklist = format_dna_norm_picklist(\n",
    "    np.array(plate_df[NORMALIZED_DNA_VOL_KEY]),\n",
    "    np.array(plate_df[NORMALIZED_WATER_VOL_KEY]),\n",
    "    np.array(plate_df[PM_WELL_KEY]),\n",
    "    dest_wells = np.array(plate_df[well_col]),\n",
    "    sample_names = np.array(plate_df[PM_SAMPLE_KEY]),\n",
    "    sample_plates = np.array(plate_df[PM_COMPRESSED_PLATE_NAME_KEY]),\n",
    "    dna_concs = np.array(plate_df[SAMPLE_DNA_CONC_KEY]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:38:43.503297Z",
     "start_time": "2025-04-01T20:38:43.499215Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write the picklist as .txt\n",
    "warn_if_fp_exists(norm_picklist_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:38:44.329046Z",
     "start_time": "2025-04-01T20:38:44.166522Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(norm_picklist_fp, 'w') as f:\n",
    "    f.write(norm_picklist)\n",
    "    \n",
    "!head {norm_picklist_fp}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 (of 5): Workflow for assigning barcodes\n",
    "\n",
    "This portion of the notebook will assign index values and construct an Echo picklist file for adding barcodes. \n",
    "\n",
    "As inputs, it requires:\n",
    "1. A plate_df dataframe (from previous step)\n",
    "2. A tab-delimited tellseq barcode file, containing Well and Barcode_ID columns\n",
    "3. The name of the tellseq barcode source plate\n",
    "\n",
    "The workflow then:\n",
    "1. reads in the tellseq barcode list\n",
    "2. assigns indices per sample\n",
    "3. produces an Echo-formatted pick list file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 of 5, Step 1 of 3: Read in tellseq barcode list\n",
    "\n",
    "This is a file that contains each unique tellseq barcode on a separate line,\n",
    "along with plate and well location information. It should look something like this:\n",
    "\n",
    "```\n",
    "Well,Barcode_96_Well_Position,Barcode_ID\n",
    "A1,A1,C501\n",
    "B1,A2,C509\n",
    "C1,B1,C502\n",
    "D1,B2,C510\n",
    "E1,C1,C503\n",
    "F1,C2,C511\n",
    "G1,D1,C504\n",
    "H1,D2,C512\n",
    "I1,E1,C505\n",
    "J1,E2,C513\n",
    "K1,F1,C506\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:38:48.607140Z",
     "start_time": "2025-04-01T20:38:48.604185Z"
    }
   },
   "outputs": [],
   "source": [
    "## INPUT\n",
    "barcodes_plate_name = 'TellSeq_Barcode_Plate_1_LN2409001_EXP052026'\n",
    "barcodes_fp = './test_data/Tellseq/TELL-Seq_Barcodes_PP_Primer_Plate - PP_Primer_Position.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:38:49.390607Z",
     "start_time": "2025-04-01T20:38:49.387772Z"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile(barcodes_fp):\n",
    "    print(\"Problem! %s is not a path to a valid file\" % barcodes_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:38:50.142603Z",
     "start_time": "2025-04-01T20:38:50.130389Z"
    }
   },
   "outputs": [],
   "source": [
    "barcodes = pd.read_csv(barcodes_fp, dtype=str)\n",
    "\n",
    "# rename the columns to match what `format_index_picklist` expects\n",
    "# and add the plate information\n",
    "barcodes.rename(columns={'Well': 'i5 well', 'Barcode_ID': 'i5 name'}, inplace=True)\n",
    "barcodes['i5 plate'] = barcodes_plate_name\n",
    "barcodes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 of 5, Step 2 of 3: Assign tellseq barcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:38:51.499477Z",
     "start_time": "2025-04-01T20:38:51.494632Z"
    }
   },
   "outputs": [],
   "source": [
    "def sort_by_col_then_row(a_df, well_key='Well'):\n",
    "    # remove the first character from the contents of a_df[well_key] and \n",
    "    # store it in its own column named f\"{well_key}_row\"\n",
    "    a_df[f\"{well_key}_row\"] = a_df[well_key].str[:1]\n",
    "    \n",
    "    # take everything BUT the first character in the contents of a_df[well_key]\n",
    "    # and convert it to an integer and store it in its own column named\n",
    "    # f\"{well_key}_col\"\n",
    "    a_df[f\"{well_key}_col\"] = a_df[well_key].str[1:].astype(int)\n",
    "    \n",
    "    # sort the dataframe first by the column and then by the row\n",
    "    a_df.sort_values(by=[f\"{well_key}_col\", f\"{well_key}_row\"], inplace=True)\n",
    "    return a_df\n",
    "\n",
    "def get_num_barcode_sets_needed(a_plate_df, barcodes_df):\n",
    "    # if num_barcode_sets_needed is not an integer, throw an error\n",
    "    num_barcode_sets_needed = a_plate_df.shape[0]/ barcodes_df.shape[0]\n",
    "    if num_barcode_sets_needed % 1 != 0:\n",
    "        raise ValueError(\n",
    "            f\"Number of barcodes ({barcodes_df.shape[0]}) \"\n",
    "            f\"does not divide evenly into number of samples \"\n",
    "            f\"]({a_plate_df.shape[0]})\")\n",
    "    return int(num_barcode_sets_needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:38:52.234681Z",
     "start_time": "2025-04-01T20:38:52.224734Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the plate_df sorted by col then row\n",
    "p_df = plate_df.copy()\n",
    "p_df = sort_by_col_then_row(p_df, well_key=PM_LIB_WELL_KEY)\n",
    "# make the existing index into a column and reindex\n",
    "p_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:38:52.819540Z",
     "start_time": "2025-04-01T20:38:52.809291Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the barcodes sorted by col then row\n",
    "b_df = barcodes.copy()\n",
    "b_df = sort_by_col_then_row(b_df, well_key='i5 well')\n",
    "b_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:38:53.373588Z",
     "start_time": "2025-04-01T20:38:53.361293Z"
    }
   },
   "outputs": [],
   "source": [
    "# make a new barcodes_sets_df that duplicates the barcodes dataframe\n",
    "# num_barcode_sets_needed times\n",
    "concat_dfs = []\n",
    "barcode_max_col = b_df['i5 well_col'].max()\n",
    "num_barcode_sets = get_num_barcode_sets_needed(p_df, b_df)\n",
    "curr_min_col = 0\n",
    "curr_max_col = barcode_max_col\n",
    "for i in range(num_barcode_sets):\n",
    "    curr_set = b_df.copy()\n",
    "    curr_set[TELLSEQ_BARCODE_SET_ID_KEY] = \\\n",
    "        f\"col{curr_min_col + 1}to{curr_max_col}\"\n",
    "    curr_min_col = curr_max_col\n",
    "    curr_max_col += barcode_max_col\n",
    "    concat_dfs.append(curr_set)\n",
    "barcode_sets_df = pd.concat(concat_dfs, ignore_index=True)\n",
    "barcode_sets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:38:54.216181Z",
     "start_time": "2025-04-01T20:38:54.196419Z"
    }
   },
   "outputs": [],
   "source": [
    "# merge the (sorted) plate_df and barcode_sets_df\n",
    "p_df = pd.merge(p_df, barcode_sets_df, \n",
    "                    left_index=True, right_index=True)\n",
    "p_df.set_index('index', inplace=True)\n",
    "p_df[TELLSEQ_BARCODE_ID_KEY] = p_df['i5 name']\n",
    "p_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:38:54.811733Z",
     "start_time": "2025-04-01T20:38:54.809130Z"
    }
   },
   "outputs": [],
   "source": [
    "plate_df = p_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 of 5, Step 3 of 3: Make barcodes pick list and write to file\n",
    "\n",
    "Format the Echo-compatible pick list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:38:56.194331Z",
     "start_time": "2025-04-01T20:38:56.191334Z"
    }
   },
   "outputs": [],
   "source": [
    "## INPUT\n",
    "barcode_picklist_fp = './test_output/Indices/Tellseq_barcode_matrix.txt'\n",
    "\n",
    "## INPUT -- verify default\n",
    "barcode_vol = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:38:57.201665Z",
     "start_time": "2025-04-01T20:38:57.142640Z"
    }
   },
   "outputs": [],
   "source": [
    "barcode_picklist = format_index_picklist(\n",
    "    plate_df[PM_SAMPLE_KEY], plate_df[well_col], barcode_sets_df,\n",
    "    i5_vol=barcode_vol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:38:57.825469Z",
     "start_time": "2025-04-01T20:38:57.822601Z"
    }
   },
   "outputs": [],
   "source": [
    "warn_if_fp_exists(barcode_picklist_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:38:58.665416Z",
     "start_time": "2025-04-01T20:38:58.520516Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(barcode_picklist_fp, 'w') as f:\n",
    "    f.write(barcode_picklist)\n",
    "\n",
    "!head {barcode_picklist_fp}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 of 5, Step 4 of 4: Write plate and study info to files\n",
    "\n",
    "We want to keep all that useful information together in one place so that\n",
    "it can be easily parsed later. Enter the base (without extension) of the two output file names; the code will provide the extensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:38:59.936306Z",
     "start_time": "2025-04-01T20:38:59.933646Z"
    }
   },
   "outputs": [],
   "source": [
    "## INPUT\n",
    "file_name_base = './test_output/QC/Tellseq'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:39:00.849856Z",
     "start_time": "2025-04-01T20:39:00.846347Z"
    }
   },
   "outputs": [],
   "source": [
    "plate_df_fp = f\"{file_name_base}_plate_df_A.txt\"\n",
    "if os.path.isfile(plate_df_fp):\n",
    "    print(\"Warning! This file exists already.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:39:01.642774Z",
     "start_time": "2025-04-01T20:39:01.627826Z"
    }
   },
   "outputs": [],
   "source": [
    "plate_df.to_csv(plate_df_fp, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the experiment and study info so it doesn't have to be re-entered by hand in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:39:03.066796Z",
     "start_time": "2025-04-01T20:39:03.063249Z"
    }
   },
   "outputs": [],
   "source": [
    "expt_info_fp = f\"{file_name_base}_expt_info.yml\"\n",
    "if os.path.isfile(expt_info_fp):\n",
    "    print(\"Warning! This file exists already.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:39:03.633821Z",
     "start_time": "2025-04-01T20:39:03.630680Z"
    }
   },
   "outputs": [],
   "source": [
    "expt_info = {\n",
    "    \"experiment_name\": expt_name,\n",
    "    \"studies\": studies_info\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T20:39:04.464239Z",
     "start_time": "2025-04-01T20:39:04.459913Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(expt_info_fp, 'w') as file:\n",
    "    yaml.dump(expt_info, file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "473px",
    "width": "381px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "740px",
    "left": "0px",
    "right": "1407.6666259765625px",
    "top": "112px",
    "width": "211.705px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
