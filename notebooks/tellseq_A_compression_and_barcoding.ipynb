{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "%reload_ext watermark\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "from scipy.stats import mannwhitneyu\n",
    "import yaml\n",
    "from metapool.metapool import *\n",
    "from metapool.util import (\n",
    "    join_dfs_from_files, extend_sample_accession_df,\n",
    "    extend_compression_layout_info, QIITA_STUDY_ID_KEY)\n",
    "from metapool.plate import PlateReplication, record_gdna_dilution\n",
    "from metapool import (add_controls, compress_plates, \n",
    "                      TUBECODE_KEY, SAMPLE_NAME_KEY, SAMPLE_DNA_CONC_KEY, \n",
    "                      NORMALIZED_DNA_VOL_KEY)\n",
    "from metapool.mp_strings import (\n",
    "    PM_SAMPLE_KEY, PM_WELL_KEY, PM_LIB_WELL_KEY, TELLSEQ_BARCODE_ID_KEY, \n",
    "    TELLSEQ_BARCODE_SET_ID_KEY)\n",
    "from metapool.util import warn_if_fp_exists\n",
    "%watermark -i -v -iv -m -h -p metapool,sample_sheet,openpyxl -u"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "! conda list",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Knight Lab TellSeq pipeline notebook"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Part 1 (of 5): Workflow for normalizing DNA\n",
    "\n",
    "This portion of the notebook will read in the output of the mini-Pico quantification assay and construct an Echo normalization picklist file. \n",
    "\n",
    "As inputs, it requires:\n",
    "1. A tab-delimited row-wise sample accession file that indicates the sample name (`sample_name`) and its associated matrix tube barcode (`TubeCode`)\n",
    "2. A tab-delimited metadata file downloaded from Qiita\n",
    "3. An accurate plate compression form, with appropriate VisionMate barcode scanner files (`Plate map file`)\n",
    "4. **TWO** DNA concentration files: one for the undiluted plate and one for the 1:10 dilution plate\n",
    "\n",
    "The workflow then:\n",
    "1. reads in the specified input files and constructs a dataframe\n",
    "2. calculates volumes to be added via echo to reach desired input DNA quantity, with info on which samples need to be pulled from the diluted plate and which from the original plate\n",
    "3. produces an Echo-formatted pick list file"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Part 1 of 5, Step 0 of 8: Provide inputs"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "expt_name = \"RKLtest\"",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# One dictionary per study included in the samples on this run.\n",
    "studies_info = [\n",
    "    # EVERY entry in the dictionary must be specifically updated \n",
    "    # *every* time this notebook is run--none of these have defaults!\n",
    "    {\n",
    "    'Project Name': 'Wellcome_Leap_15538', # PROJECTNAME_QIITAID\n",
    "    'Project Abbreviation': 'WellcomeLeap', # PROJECTNAME\n",
    "    'sample_accession_fp': './test_data/Plate_Maps/Tellseq_Wellcome Leap - 15538 - Sample Accession.csv',\n",
    "    'qiita_metadata_fp': './test_data/Plate_Maps/15538_20241004-110731.txt',\n",
    "    'experiment_design_description': 'isolate sequencing',\n",
    "    'HumanFiltering': 'False', \n",
    "    'Email': 'r@gmail.com'\n",
    "    }\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: ask what you put in here when doing replicates\n",
    "compression_layout = [\n",
    "    {\n",
    "        # top left plate\n",
    "        'Plate Position': 1, # as int\n",
    "        'Plate map file': './test_data/Plate_Maps/Tellseq_Test_Plate_1.tsv',\n",
    "        'Project Name': 'Wellcome_Leap_15538', # PROJECTNAME_QIITAID\n",
    "        'Project Plate': 'Plate_1', # Plate_#\n",
    "        'Plate elution volume': 70\n",
    "    },\n",
    "    {\n",
    "        # top right plate\n",
    "        'Plate Position': 2, # as int\n",
    "        'Plate map file': './test_data/Plate_Maps/Tellseq_Test_Plate_2.tsv',\n",
    "        'Project Name': 'Wellcome_Leap_15538', # PROJECTNAME_QIITAID\n",
    "        'Project Plate': 'Plate_2', # Plate_#\n",
    "        'Plate elution volume': 70\n",
    "    },\n",
    "    {\n",
    "        # bottom left plate\n",
    "        'Plate Position': 3, # as int\n",
    "        'Plate map file': './test_data/Plate_Maps/Tellseq_Test_Plate_3.tsv',\n",
    "        'Project Name': 'Wellcome_Leap_15538', # PROJECTNAME_QIITAID\n",
    "        'Project Plate': 'Plate_3', # Plate_#\n",
    "        'Plate elution volume': 70\n",
    "    },\n",
    "    {\n",
    "        # bottom right plate\n",
    "        'Plate Position': 4, # as int\n",
    "        'Plate map file': './test_data/Plate_Maps/Tellseq_Test_Plate_4.tsv',\n",
    "        'Project Name': 'Wellcome_Leap_15538', # PROJECTNAME_QIITAID \n",
    "        'Project Plate': 'Plate_4',  # Plate_#\n",
    "        'Plate elution volume': 70\n",
    "    },\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# CONSTANTS: Users, DO NOT CHANGE THESE\n",
    "# values without consulting with tech team\n",
    "\n",
    "# Mask arrays for even and odd rows and columns\n",
    "EVEN_ROWS = [x for x in range(16) if x % 2 == 0]\n",
    "ODD_ROWS = [x for x in range(16) if x % 2 == 1]\n",
    "EVEN_COLS = [x for x in range(24) if x % 2 == 0]\n",
    "ODD_COLS = [x for x in range(24) if x % 2 == 1]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_studies_attr_list(studies_dict, desired_key):\n",
    "    return [x[desired_key] for x in studies_dict]\n",
    "\n",
    "def pick_expected_separator(fps_list):\n",
    "    sep = \"\\t\"\n",
    "    visible_sep = \"tab\"\n",
    "    \n",
    "    num_fps = len(fps_list)\n",
    "    num_csv = sum([x.endswith('.csv') for x in fps_list])\n",
    "    num_txt = sum([x.endswith('.txt') for x in fps_list])\n",
    "    num_tsv = sum([x.endswith('.tsv') for x in fps_list])\n",
    "    \n",
    "    if num_csv == num_fps:\n",
    "        sep = ','\n",
    "        visible_sep = \"comma\"\n",
    "    elif (num_tsv + num_txt) != num_fps:\n",
    "        warnings.warn(\n",
    "            \"Could not determine separator; defaulting to \" + visible_sep)\n",
    "\n",
    "    return sep, visible_sep"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Part 1 of 5, Step 1 of 8: Read in sample accession files"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# read in the sample accession files\n",
    "sample_accession_fps = get_studies_attr_list(\n",
    "    studies_info, 'sample_accession_fp')\n",
    "sample_acc_sep, sa_sep_name = pick_expected_separator(sample_accession_fps)\n",
    "print(f\"Expected sample accession separator: {sa_sep_name}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sample_accession_df = join_dfs_from_files(\n",
    "    sample_accession_fps, [SAMPLE_NAME_KEY, TUBECODE_KEY], sep=sample_acc_sep)\n",
    "sample_accession_df.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sample_accession_df.head()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Part 1 of 5, Step 2 of 8: Read in the sample info from Qiita"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# read in the qiita metadata files\n",
    "qiita_metadata_fps = get_studies_attr_list(studies_info, 'qiita_metadata_fp')\n",
    "qiita_metadata_sep, qm_sep_name = pick_expected_separator(qiita_metadata_fps)\n",
    "print(f\"Expected qiita metadata separator: {qm_sep_name}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "metadata_df = join_dfs_from_files(\n",
    "    qiita_metadata_fps, [SAMPLE_NAME_KEY, QIITA_STUDY_ID_KEY], \n",
    "    opt_cols_to_extract=['tube_id'], unique_cols=[SAMPLE_NAME_KEY],\n",
    "    sep=qiita_metadata_sep)\n",
    "metadata_df.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "metadata_df.head()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now use the metadata to link the study info into the sample accession dataframe:"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "extended_sample_accession_df = extend_sample_accession_df(\n",
    "    sample_accession_df, studies_info, metadata_df)\n",
    "extended_sample_accession_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Part 1 of 5, Step 3 of 8: Assign the compression layout and add controls"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# copy study info into the compression layout dictionary (so that it doesn't \n",
    "# have to be entered manually in both places)\n",
    "extended_compression_layout = extend_compression_layout_info(\n",
    "    compression_layout, studies_info)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plate_df = compress_plates(extended_compression_layout, \n",
    "                           extended_sample_accession_df, well_col=PM_WELL_KEY)\n",
    "plate_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Check for samples with missing names; at this point, we expect all blanks\n",
    "and katharoseq controls WON'T have names."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def check_nan_samples(a_plate_df, a_blanks_dir=None):\n",
    "    num_remaining_nans = a_plate_df[a_plate_df[PM_SAMPLE_KEY].isna()].shape[0]\n",
    "    print(\"Number of samples with missing names: %d\" % num_remaining_nans)\n",
    "    \n",
    "    if num_remaining_nans > 0 and a_blanks_dir:\n",
    "        err_msg = f\"\"\"\n",
    "By now, all samples should have names, so **do not continue** before fixing this!\n",
    "\n",
    "\"Unofficial\" blanks are the most likely issue.\n",
    "Determine if the tube codes for the problem samples (shown below) are blanks.\n",
    "If they are, add them to the missing_blanks.csv file in the {a_blanks_dir} directory.\n",
    "Then re-run from 'Part 1 of 5, Step 3 of 8: Assign the compression layout and add controls'.\"\"\"\n",
    "        print(err_msg)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "check_nan_samples(plate_df)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "blanks_dir = './test_data/BLANKS_for_tellseq'\n",
    "# ATTENTION: Does your plate include katharoseq controls?\n",
    "# If *yes*, replace the None below with the path to the directory they are in, such as\n",
    "# katharoseq_dir = './test_data/katharoseq'\n",
    "katharoseq_dir = None\n",
    "\n",
    "plate_df = add_controls(plate_df, blanks_dir, katharoseq_dir)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "After adding controls, check again for samples with missing names; \n",
    "at this point, we expect all blanks and katharoseq controls WILL have names, \n",
    "so if there are any remaining samples without names, \n",
    "stop processing and fix them!"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "check_nan_samples(plate_df, a_blanks_dir=blanks_dir)\n",
    "plate_df[plate_df[PM_SAMPLE_KEY].isna()]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Part 1 of 5, Step 4 of 8: Validate plate dataframe"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# note that this function does not *need* the extended sample accession df,\n",
    "# but it is easier to use it just to keep things consistent\n",
    "validate_plate_df(plate_df,metadata_df, extended_sample_accession_df, \n",
    "                  blanks_dir, katharoseq_dir)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 of 5, Step 5 of 8: read in DNA concentrations and add to plate map\n",
    "\n",
    "Enter the path to each of the Pico DNA concentration output files. Each one should be\n",
    " a tab-separated file produced by the MiniPico assay on the condensed, \n",
    " 384-well plate, and should have a format like the below:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "```\n",
    "##BLOCKS= 1\n",
    "Group: Unknowns\n",
    "Sample\tWells\tRFU_Values\tConcentration\tMean_Conc\tSD\tCV\tDilution\tAdjConc\t\n",
    "01\tA1\t528791.000\t2.472\t2.472\t0.000\t0.0\t\t\t\n",
    "02\tC1\t481728.000\t2.282\t2.282\t0.000\t0.0\t\t\t\n",
    "03\tE1\t462964.000\t2.206\t2.206\t0.000\t0.0\t\t\t\n",
    "04\tG1\t556609.000\t2.585\t2.585\t0.000\t0.0\t\t\t\n",
    "05\tI1\t710679.000\t3.207\t3.207\t0.000\t0.0\t\t\t\n",
    "06\tK1\t655693.000\t2.985\t2.985\t0.000\t0.0\t\t\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ORIGINAL (undiluted) gDNA concentration file\n",
    "sample_concs_fp =  './test_data/Quant/MiniPico/Tellseq_gDNA_Original_Quant.txt'\n",
    "\n",
    "# 1:10 diluted gDNA concentration file\n",
    "diluted_sample_concs_fp = './test_data/Quant/MiniPico/Tellseq_gDNA_diluted_10_to_1_Quant.txt'\n",
    "\n",
    "for curr_fp in [sample_concs_fp, diluted_sample_concs_fp]:\n",
    "    if not os.path.isfile(curr_fp):\n",
    "        print(\"Problem! %s is not a path to a valid file\" % curr_fp)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "DILUTED_SUFFIX = \"_diluted\"\n",
    "UNDILUTED_SUFFIX = \"_undiluted\"\n",
    "UNDILUTED_CONC_KEY = f\"{SAMPLE_DNA_CONC_KEY}{UNDILUTED_SUFFIX}\"\n",
    "DILUTED_CONC_KEY = f\"{SAMPLE_DNA_CONC_KEY}{DILUTED_SUFFIX}\"\n",
    "\n",
    "def read_agnostic_pico_csv(a_fp, name_suffix, plate_reader):\n",
    "    a_df = read_pico_csv(a_fp, plate_reader=plate_reader)\n",
    "    suffixed_names = {x: f\"{x}{name_suffix}\" for x in a_df.columns}\n",
    "    suffixed_names.pop(PM_WELL_KEY)  # Don't actually want to rename that :)\n",
    "    a_df.rename(columns=suffixed_names, inplace=True)\n",
    "    return a_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sample_concs = read_agnostic_pico_csv(\n",
    "    sample_concs_fp, UNDILUTED_SUFFIX, plate_reader='SpectraMax_i3x')\n",
    "sample_concs.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "diluted_sample_concs = read_agnostic_pico_csv(\n",
    "    diluted_sample_concs_fp, DILUTED_SUFFIX, 'SpectraMax_i3x')\n",
    "diluted_sample_concs.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plate_df = pd.merge(plate_df, sample_concs, on=PM_WELL_KEY)\n",
    "plate_df = pd.merge(plate_df, diluted_sample_concs, on=PM_WELL_KEY)\n",
    "plate_df[SAMPLE_DNA_CONC_KEY] = plate_df[UNDILUTED_CONC_KEY]  # default\n",
    "plate_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Add dilution info into the plate df; for every sample with a DILUTED \n",
    "# concentration greater than or equal to the min concentration threshold, we\n",
    "# SHOULD use the diluted plate values.\n",
    "min_conc_threshold = 1.5  # ng/ul"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "diluted_mask = plate_df[DILUTED_CONC_KEY] >= min_conc_threshold\n",
    "plate_df = record_gdna_dilution(plate_df, diluted_mask, DILUTED_CONC_KEY)\n",
    "plate_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize plate DNA concentrations and plate map:**\n",
    "\n",
    "Undiluted concentrations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# get DNA concentration information\n",
    "undiluted_dna_concs = make_2D_array(plate_df, data_col=UNDILUTED_CONC_KEY, \n",
    "                          well_col=PM_WELL_KEY).astype(float)\n",
    "\n",
    "# get information for annotation\n",
    "names = make_2D_array(plate_df, data_col=PM_SAMPLE_KEY, well_col=PM_WELL_KEY)\n",
    "\n",
    "plot_plate_vals(undiluted_dna_concs,\n",
    "                annot_str=names,\n",
    "                color_map='viridis',\n",
    "                annot_fmt='.5s')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Diluted concentrations"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# get DNA concentration information\n",
    "diluted_dna_concs = make_2D_array(plate_df, data_col=DILUTED_CONC_KEY, \n",
    "                                  well_col=PM_WELL_KEY).astype(float)\n",
    "\n",
    "# get information for annotation\n",
    "diluted_names = make_2D_array(plate_df, data_col=PM_SAMPLE_KEY, well_col=PM_WELL_KEY)\n",
    "\n",
    "plot_plate_vals(diluted_dna_concs,\n",
    "                annot_str=diluted_names,\n",
    "                color_map='viridis',\n",
    "                annot_fmt='.5s')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Make sample replicates\n",
    "\n",
    "Set replicate dictionary, if needed."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Replicate formats:\n",
    "# replicate_dict = {source1_quadrant:destination1_quadrant}\n",
    "# replicate_dict = {source1_quadrant:[destination1_quadrants,destination1_quadrants]}\n",
    "# Replicate example: \n",
    "# replicate_dict = {1:[2,3]}\n",
    "# for no replicates, use:\n",
    "replicate_dict = None\n",
    "\n",
    "# 'Well' differs from 'Library Well' because the former specifies the \n",
    "# gDNA source well while the latter specifies the well (destination well) that \n",
    "# will contain the sequencing library for the sample. These contain the same\n",
    "# info when replicates are not used, but differ when replicates ARE used,\n",
    "# so it is safer to use 'Library Well' in both cases.\n",
    "# (Careful!  well_col is a global variable used throughout rest of notebook)\n",
    "well_col = PM_LIB_WELL_KEY"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# initialize new PlateReplication object to manage metadata, conversions, etc.\n",
    "# initialize w/preferred well_col.\n",
    "pr = PlateReplication(well_col)\n",
    "\n",
    "# set overwrite=False to detect any overwriting of source or destination quads \n",
    "# and raise an Error.\n",
    "plate_df = pr.make_replicates(\n",
    "    plate_df, replicates=replicate_dict, overwrite=True)\n",
    "\n",
    "# replicates overlapping sample_wells for other samples should raise warning,\n",
    "# but will be allowed\n",
    "if 'True' in plate_df['contains_replicates'].unique():\n",
    "    raise NotImplementedError(\"This notebook does not yet support replicates.\")\n",
    "    \n",
    "    # plate_df['contains_replicates'] = True\n",
    "    # # get DNA concentration information\n",
    "    # dna_concs = make_2D_array(plate_df, data_col='Sample DNA Concentration', \n",
    "    #                           well_col=well_col).astype(float)\n",
    "    # \n",
    "    # # get information for annotation\n",
    "    # names = make_2D_array(plate_df, data_col=PM_SAMPLE_KEY, well_col=well_col)\n",
    "    # \n",
    "    # plot_plate_vals(dna_concs,\n",
    "    #             annot_str=names,\n",
    "    #             color_map='viridis',\n",
    "    #             annot_fmt='.6s')\n",
    "else:\n",
    "    plate_df['contains_replicates'] = False\n",
    "    \n",
    "# show whether this plate contains replicates or not\n",
    "f\"Contains replicates: {plate_df['contains_replicates'].unique()}\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### gDNA concentration heatmap, Plate 1"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plot_plate_vals(undiluted_dna_concs[np.ix_(EVEN_ROWS,EVEN_COLS)],\n",
    "                annot_str= names[np.ix_(EVEN_ROWS,EVEN_COLS)],\n",
    "                color_map='viridis',\n",
    "                annot_fmt='')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### gDNA concentration heatmap, Plate 2"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plot_plate_vals(undiluted_dna_concs[np.ix_(EVEN_ROWS,ODD_COLS)],\n",
    "                    annot_str= names[np.ix_(EVEN_ROWS,ODD_COLS)],\n",
    "                    color_map='viridis',\n",
    "                    annot_fmt='')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### gDNA concentration heatmap, Plate 3"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plot_plate_vals(undiluted_dna_concs[np.ix_(ODD_ROWS,EVEN_COLS)],\n",
    "                    annot_str= names[np.ix_(ODD_ROWS,EVEN_COLS)],\n",
    "                    color_map='viridis',\n",
    "                    annot_fmt='')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### gDNA concentration heatmap, Plate 4"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plot_plate_vals(undiluted_dna_concs[np.ix_(ODD_ROWS,ODD_COLS)],\n",
    "                    annot_str= names[np.ix_(ODD_ROWS,ODD_COLS)],\n",
    "                    color_map='viridis',\n",
    "                    annot_fmt='')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 of 5, Step 6 of 8: calculate normalization volumes and add to plate map\n",
    "\n",
    "This step will calculate volumes for the DNA normalization pick list.\n",
    "\n",
    "Check the desired values for:\n",
    " - **`ng`**: the desired quantity of DNA in normed plate, in ng\n",
    " - **`total_vol`**: the total volume of normalized DNA, in nL\n",
    " - **`min_vol`**: the minimum quantity of sample to add, in nL\n",
    " - **`resolution`**: the resolution of the Echo, in nL (usually 2.5)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ng = 7.5\n",
    "total_vol = 5000\n",
    "min_vol = 25\n",
    "resolution = 2.5"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "NORMALIZED_WATER_VOL_KEY = 'Normalized water volume'\n",
    "\n",
    "dna_vols = calculate_norm_vol(\n",
    "    plate_df[SAMPLE_DNA_CONC_KEY], ng=ng, min_vol=min_vol, \n",
    "    max_vol=total_vol, resolution=resolution)\n",
    "water_vols = total_vol - dna_vols\n",
    "\n",
    "plate_df[NORMALIZED_DNA_VOL_KEY] = dna_vols\n",
    "plate_df[NORMALIZED_WATER_VOL_KEY] = water_vols\n",
    "plate_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Part 1 of 5, Step 7 of 8 (optional): Add synDNA spike-in"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Set syndna_pool_number to 1 if syndna is being used; otherwise, leave as None\n",
    "syndna_pool_number = None"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plate_df = add_syndna(plate_df, \n",
    "                      syndna_pool_number=syndna_pool_number,\n",
    "                      syndna_concentration=2.22)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "f'For this plate, is_absquant = {is_absquant(plate_df)}'",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if is_absquant(plate_df):\n",
    "    syndna_well='A1'\n",
    "    syndna_plate = 'synDNA plate'\n",
    "    syndna_picklist = \\\n",
    "        format_dna_norm_picklist(\n",
    "            np.array(plate_df['synDNA volume']),\n",
    "            np.zeros(plate_df.shape[0]),\n",
    "            np.repeat(syndna_well,plate_df.shape[0]),\n",
    "            dest_wells = np.array(plate_df[well_col]),\n",
    "            sample_names = np.array(plate_df[PM_SAMPLE_KEY]),\n",
    "            sample_plates = np.repeat(syndna_plate,plate_df.shape[0]))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if is_absquant(plate_df):\n",
    "    # Write the picklist as .txt\n",
    "    syndna_picklist_fp = './test_output/Input_Norm/Tellseq_matrix_syndna_absquant.txt'\n",
    "\n",
    "    if os.path.isfile(syndna_picklist_fp):\n",
    "        print(\"Warning! This file exists already.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if is_absquant(plate_df):\n",
    "    with open(syndna_picklist_fp, 'w') as f:\n",
    "        f.write(syndna_picklist)\n",
    "\n",
    "    !head {syndna_picklist_fp}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 of 5, Step 8 of 8: Make pick list and write to file\n",
    "\n",
    "Format the Echo-compatible pick list."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "norm_picklist = format_dna_norm_picklist(\n",
    "    np.array(plate_df[NORMALIZED_DNA_VOL_KEY]),\n",
    "    np.array(plate_df[NORMALIZED_WATER_VOL_KEY]),\n",
    "    np.array(plate_df[PM_WELL_KEY]),\n",
    "    dest_wells = np.array(plate_df[well_col]),\n",
    "    sample_names = np.array(plate_df[PM_SAMPLE_KEY]),\n",
    "    sample_plates = np.array(plate_df[PM_COMPRESSED_PLATE_NAME_KEY]),\n",
    "    dna_concs = np.array(plate_df[SAMPLE_DNA_CONC_KEY]))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write the picklist as .txt\n",
    "norm_picklist_fp = './test_output/Input_Norm/Tellseq_inputnorm.txt'\n",
    "warn_if_fp_exists(norm_picklist_fp)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with open(norm_picklist_fp, 'w') as f:\n",
    "    f.write(norm_picklist)\n",
    "    \n",
    "!head {norm_picklist_fp}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Part 2 (of 5): Workflow for assigning barcodes\n",
    "\n",
    "This portion of the notebook will assign index values and construct an Echo picklist file for adding barcodes. \n",
    "\n",
    "As inputs, it requires:\n",
    "1. A plate_df dataframe (from previous step)\n",
    "2. A tab-delimited tellseq barcode file, containing Well and Barcode_ID columns\n",
    "3. The name of the tellseq barcode source plate\n",
    "\n",
    "The workflow then:\n",
    "1. reads in the tellseq barcode list\n",
    "2. assigns indices per sample\n",
    "3. produces an Echo-formatted pick list file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 of 5, Step 1 of 3: Read in tellseq barcode list\n",
    "\n",
    "This is a file that contains each unique tellseq barcode on a separate line,\n",
    "along with plate and well location information. It should look something like this:\n",
    "\n",
    "```\n",
    "Well,Barcode_96_Well_Position,Barcode_ID\n",
    "A1,A1,C501\n",
    "B1,A2,C509\n",
    "C1,B1,C502\n",
    "D1,B2,C510\n",
    "E1,C1,C503\n",
    "F1,C2,C511\n",
    "G1,D1,C504\n",
    "H1,D2,C512\n",
    "I1,E1,C505\n",
    "J1,E2,C513\n",
    "K1,F1,C506\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "barcodes_plate_name = 'TellSeq_Barcode_Plate_1_LN2409001_EXP052026'\n",
    "barcodes_fp = './test_data/Tellseq/TELL-Seq_Barcodes_PP_Primer_Plate - PP_Primer_Position.csv'\n",
    "\n",
    "if not os.path.isfile(barcodes_fp):\n",
    "    print(\"Problem! %s is not a path to a valid file\" % barcodes_fp)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "barcodes = pd.read_csv(barcodes_fp, dtype=str)\n",
    "\n",
    "# rename the columns to match what `format_index_picklist` expects\n",
    "# and add the plate information\n",
    "barcodes.rename(columns={'Well': 'i5 well', 'Barcode_ID': 'i5 name'}, inplace=True)\n",
    "barcodes['i5 plate'] = barcodes_plate_name\n",
    "barcodes.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Part 2 of 5, Step 2 of 3: Assign tellseq barcodes"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def sort_by_col_then_row(a_df, well_key='Well'):\n",
    "    # remove the first character from the contents of a_df[well_key] and \n",
    "    # store it in its own column named f\"{well_key}_row\"\n",
    "    a_df[f\"{well_key}_row\"] = a_df[well_key].str[:1]\n",
    "    \n",
    "    # take everything BUT the first character in the contents of a_df[well_key]\n",
    "    # and convert it to an integer and store it in its own column named\n",
    "    # f\"{well_key}_col\"\n",
    "    a_df[f\"{well_key}_col\"] = a_df[well_key].str[1:].astype(int)\n",
    "    \n",
    "    # sort the dataframe first by the column and then by the row\n",
    "    a_df.sort_values(by=[f\"{well_key}_col\", f\"{well_key}_row\"], inplace=True)\n",
    "    return a_df\n",
    "\n",
    "def get_num_barcode_sets_needed(a_plate_df, barcodes_df):\n",
    "    # if num_barcode_sets_needed is not an integer, throw an error\n",
    "    num_barcode_sets_needed = a_plate_df.shape[0]/ barcodes_df.shape[0]\n",
    "    if num_barcode_sets_needed % 1 != 0:\n",
    "        raise ValueError(\n",
    "            f\"Number of barcodes ({barcodes_df.shape[0]}) \"\n",
    "            f\"does not divide evenly into number of samples \"\n",
    "            f\"]({a_plate_df.shape[0]})\")\n",
    "    return int(num_barcode_sets_needed)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# get the plate_df sorted by col then row\n",
    "p_df = plate_df.copy()\n",
    "p_df = sort_by_col_then_row(p_df, well_key=PM_LIB_WELL_KEY)\n",
    "# make the existing index into a column and reindex\n",
    "p_df.reset_index(inplace=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# get the barcodes sorted by col then row\n",
    "b_df = barcodes.copy()\n",
    "b_df = sort_by_col_then_row(b_df, well_key='i5 well')\n",
    "b_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# make a new barcodes_sets_df that duplicates the barcodes dataframe\n",
    "# num_barcode_sets_needed times\n",
    "concat_dfs = []\n",
    "barcode_max_col = b_df['i5 well_col'].max()\n",
    "num_barcode_sets = get_num_barcode_sets_needed(p_df, b_df)\n",
    "curr_min_col = 0\n",
    "curr_max_col = barcode_max_col\n",
    "for i in range(num_barcode_sets):\n",
    "    curr_set = b_df.copy()\n",
    "    curr_set[TELLSEQ_BARCODE_SET_ID_KEY] = \\\n",
    "        f\"col{curr_min_col + 1}to{curr_max_col}\"\n",
    "    curr_min_col = curr_max_col\n",
    "    curr_max_col += barcode_max_col\n",
    "    concat_dfs.append(curr_set)\n",
    "barcode_sets_df = pd.concat(concat_dfs, ignore_index=True)\n",
    "barcode_sets_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# merge the (sorted) plate_df and barcode_sets_df\n",
    "p_df = pd.merge(p_df, barcode_sets_df, \n",
    "                    left_index=True, right_index=True)\n",
    "p_df.set_index('index', inplace=True)\n",
    "p_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plate_df = p_df",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 of 5, Step 3 of 3: Make barcodes pick list and write to file\n",
    "\n",
    "Format the Echo-compatible pick list."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "barcode_vol = 4000",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "barcode_picklist = format_index_picklist(\n",
    "    plate_df[PM_SAMPLE_KEY], plate_df[well_col], barcode_sets_df,\n",
    "    i5_vol=barcode_vol)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write the picklist as .txt\n",
    "barcode_picklist_fp = './test_output/Indices/Tellseq_barcode_matrix.txt'\n",
    "warn_if_fp_exists(barcode_picklist_fp)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with open(barcode_picklist_fp, 'w') as f:\n",
    "    f.write(barcode_picklist)\n",
    "\n",
    "!head {barcode_picklist_fp}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 (of 5): Library concentration estimation\n",
    "\n",
    "This portion of the notebook takes in fluorescent\n",
    " quantification values and produces visual outputs to interpret and check \n",
    " values. \n",
    "\n",
    "As inputs, this workflow requires:\n",
    "1. A plate map DataFrame (from previous step)\n",
    "2. MiniPico output (tab-delimited text format with columns 'Concentration' and 'Well')\n",
    "\n",
    "The workflow:\n",
    "1. reads in MiniPico output and calculates estimated library concentration\n",
    "2. visualizes concentration\n",
    "3. outputs a plate file and a studies info file for later use in per-barcode-set pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 of 5, Step 1 of 4: read in MiniPico library concentration\n",
    "Enter path to MiniPico file:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "lib_concs_fp = './test_data/Quant/MiniPico/Tellseq_clean_lib_quant.txt'",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "lib_concs = read_pico_csv(lib_concs_fp, plate_reader='SpectraMax_i3x',\n",
    "                          conc_col_name='MiniPico Library DNA Concentration')\n",
    "lib_concs.rename(columns={'Well':well_col},inplace=True)\n",
    "plate_df = pd.merge(plate_df, lib_concs, on=well_col)\n",
    "\n",
    "plate_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 of 5, Step 2 of 4: calculate sample concentration from MiniPico\n",
    "\n",
    "You will want to make sure that 'size' is correct for your average library size."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plate_df['MiniPico Library Concentration'] = \\\n",
    "    compute_pico_concentration(\n",
    "        plate_df['MiniPico Library DNA Concentration'], size=500)\n",
    "plate_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 of 5, Step 3 of 4: visualize MiniPico values\n",
    "\n",
    "This step will present visuals of the results, including:\n",
    "1. Scatter plot of DNA concentrations by Library concentration\n",
    "2. Plate-wise heatmap and histogram showing library concentrations\n",
    "3. per-96-well plate heatmaps and histograms showing library concentrations and sample names\n",
    "4. Plate-wise heatmap showing pooling values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Library concentration by sample DNA concentration:"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "f, (ax1,ax2,ax3) = plt.subplots(nrows=1, ncols=3, figsize=(14, 4))\n",
    "plate_df['Input DNA'] = plate_df['Sample DNA Concentration']*plate_df['Normalized DNA volume']/1000\n",
    "sns.regplot(x=\"Sample DNA Concentration\", y=\"MiniPico Library DNA Concentration\", data=plate_df, ax = ax1)\n",
    "sns.boxplot(x=\"Blank\", y=\"MiniPico Library DNA Concentration\", data=plate_df, ax = ax2)\n",
    "sns.swarmplot(x=\"Blank\", y=\"MiniPico Library DNA Concentration\", data=plate_df, ax = ax2,\n",
    "              size=3,color='black',alpha=0.5)\n",
    "sns.scatterplot( x=\"Input DNA\",y=\"MiniPico Library DNA Concentration\",hue='Sample DNA Concentration',data=plate_df ,ax = ax3)\n",
    "ax3.legend(title='Sample DNA Concentration',loc='center left', bbox_to_anchor=(1, 0.5))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "blanks_gdna_concs = plate_df.loc[plate_df['Blank']==True,'Sample DNA Concentration']\n",
    "samples_gdna_concs = plate_df.loc[plate_df['Blank']==False,'Sample DNA Concentration']\n",
    "mannwhitneyu(samples_gdna_concs, blanks_gdna_concs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "blanks_lib_concs = plate_df.loc[plate_df['Blank']==True,'MiniPico Library Concentration']\n",
    "samples_lib_concs = plate_df.loc[plate_df['Blank']==False,'MiniPico Library Concentration']\n",
    "mannwhitneyu(samples_lib_concs, blanks_lib_concs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Library concentration heatmap, whole plate"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# get concentration and pooling values for plotting\n",
    "concs = make_2D_array(plate_df, data_col=\"MiniPico Library Concentration\", well_col=well_col).astype(float)\n",
    "dna = make_2D_array(plate_df, data_col=SAMPLE_DNA_CONC_KEY, well_col=well_col).astype(float)\n",
    "\n",
    "# get information for annotation\n",
    "names = make_2D_array(plate_df, data_col=PM_SAMPLE_KEY, well_col=well_col)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "plot_plate_vals(concs, color_map='viridis')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Plate maps for individual constituent plates"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "##### Library concentration heatmap, Plate 1"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plot_plate_vals(concs[np.ix_(EVEN_ROWS,EVEN_COLS)],\n",
    "                    annot_str= names[np.ix_(EVEN_ROWS,EVEN_COLS)],\n",
    "                    color_map='viridis',\n",
    "                    annot_fmt='')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "##### Library concentration heatmap, Plate 2"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plot_plate_vals(concs[np.ix_(EVEN_ROWS,ODD_COLS)],\n",
    "                    annot_str= names[np.ix_(EVEN_ROWS,ODD_COLS)],\n",
    "                    color_map='viridis',\n",
    "                    annot_fmt='')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "##### Library concentration heatmap, Plate 3"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plot_plate_vals(concs[np.ix_(ODD_ROWS,EVEN_COLS)],\n",
    "                    annot_str= names[np.ix_(ODD_ROWS,EVEN_COLS)],\n",
    "                    color_map='viridis',\n",
    "                    annot_fmt='')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "##### Library concentration heatmap, Plate 4"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plot_plate_vals(concs[np.ix_(ODD_ROWS,ODD_COLS)],\n",
    "                    annot_str= names[np.ix_(ODD_ROWS,ODD_COLS)],\n",
    "                    color_map='viridis',\n",
    "                    annot_fmt='')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Part 3 of 5, Step 4 of 4: Write plate and study info to files\n",
    "\n",
    "We want to keep all that useful information together in one place so that\n",
    "it can be easily parsed later. Enter the base (without extension) of the two output file names; the code will provide the extensions."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plate_df_fbase = './test_output/QC/Tellseq_plate_df'\n",
    "expt_info_fbase = './test_output/QC/Tellseq_expt_info'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Add final columns to plate df, then save to a file."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plate_df['sample sheet Sample_ID'] = \\\n",
    "    plate_df[PM_SAMPLE_KEY].map(bcl_scrub_name)\n",
    "plate_df[TELLSEQ_BARCODE_ID_KEY] = plate_df['i5 name']\n",
    "plate_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plate_df_fp = f\"{plate_df_fbase}_A.txt\"\n",
    "if os.path.isfile(plate_df_fp):\n",
    "    print(\"Warning! This file exists already.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plate_df.to_csv(plate_df_fp, sep='\\t')",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Save the experiment and study info so it doesn't have to be re-entered by hand in the next notebook."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "expt_info_fp = f\"{expt_info_fbase}.yml\"\n",
    "if os.path.isfile(expt_info_fp):\n",
    "    print(\"Warning! This file exists already.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "expt_info = {\n",
    "    \"experiment_name\": expt_name,\n",
    "    \"studies\": studies_info\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(expt_info_fp, 'w') as file:\n",
    "    yaml.dump(expt_info, file, default_flow_style=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "473px",
    "width": "381px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "740px",
    "left": "0px",
    "right": "1407.6666259765625px",
    "top": "112px",
    "width": "211.705px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
