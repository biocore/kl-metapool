{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%reload_ext watermark\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "from scipy.stats import mannwhitneyu\n",
    "from contextlib import suppress\n",
    "from metapool.metapool import *\n",
    "from metapool.util import (\n",
    "    join_dfs_from_files, extend_sample_accession_df,\n",
    "    extend_compression_layout_info, QIITA_STUDY_ID_KEY)\n",
    "from metapool.plate import PlateReplication\n",
    "from metapool import (make_sample_sheet, requires_dilution, dilute_gDNA,\n",
    "                      find_threshold, autopool, add_controls, compress_plates, \n",
    "                      TUBECODE_KEY, SAMPLE_NAME_KEY)\n",
    "from metapool.mp_strings import PM_SAMPLE_KEY\n",
    "from metapool.sample_sheet import (\n",
    "    STANDARD_METAG_SHEET_TYPE, ABSQUANT_SHEET_TYPE, make_sections_dict)\n",
    "%watermark -i -v -iv -m -h -p metapool,sample_sheet,openpyxl -u"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "! conda list",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knight Lab shotgun pipeline notebook\n",
    "\n",
    "### What is it?\n",
    "\n",
    "This Jupyter Notebook allows you to automatically produce most of the files you need for completing the Knight Lab shotgun sequencing pipeline.\n",
    "\n",
    "Hopefully, this will not only make it much easier to generate these files, but also keep our information more accurate and tractable.\n",
    "\n",
    "### Here's how it should work.\n",
    "\n",
    "You'll start out with a **sample accession file**, which links each sample to it's appropriate matrix tube barcode. Then you'll read in the sample information from a Qiita metadata file. \n",
    "\n",
    "Next, you'll use a Compression Layout form to generate a 384-well dataframe of the sample well-locations and associated extraction plate metadata [`Project Name`, `Project Plate`, `Project Abbreviation`, `Plate elution volume`] The Compression Layout form will import 96-well plate layouts from **VisionMate plate reader output files**, and will assign a 384-well sample well-location based on the 384-well quadrant (`Plate Position`) each 96-well plate is occupying. Then, you'll compare the matrix tube barcodes in your 384-well plate with those stored in folders documenting control matrix tubes to automatically assign controls using the add_controls() function, followed by a validation step. \n",
    "\n",
    "Next, you will merge the sample gDNA concentration **quantification file** from the MiniPico assay, which will enable to you to automatically make a **normalization pick list** for starting the shotgun library prep itself. You can also visualize these concentrations on the plate, allowing you to double check the plate map and gDNA concentration read.\n",
    "\n",
    "You'll then automatically assign barcodes to each sample by specifying a **plate counter**, producing a unique **index pick list** for barcode addition prior to PCR.\n",
    "\n",
    "After finishing the shotgun library prep itself, you'll measure library concentration with the MiniPico assay. The sequencing library concentration **quantification file** will then be merged and used to estimate and visualize pooling parameters, producing a **pooling pick list**. \n",
    "\n",
    "Then, the per-sample information from the whole run can be combined to automatically produce **sample sheets** that you can use to demultiplex the sequencing data produced by Illumina sequencers. You'll need to specify the **sequencing platform** in order to produce an accurate sample sheet. \n",
    "\n",
    "Finally, you'll merge the sequence counts associated with each sample from a **sequence counts file** and produce a **sequence count normalized pooling pick list** for a final high output sequencing run. "
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Part 1 (of 4): Workflow for normalizing DNA\n",
    "\n",
    "This portion of the notebook will read in the output of the mini-Pico quantification assay and construct an Echo normalization picklist file. \n",
    "\n",
    "As inputs, it requires:\n",
    "1. A tab-delimited row-wise sample accession file that indicates the sample name (`sample_name`) and its associated matrix tube barcode (`TubeCode`)\n",
    "2. A tab-delimited metadata file downloaded from Qiita\n",
    "3. An accurate plate compression form, with appropriate VisionMate barcode scanner files (`Plate map file`)\n",
    "\n",
    "The workflow then:\n",
    "1. reads in the specified input files and constructs a dataframe\n",
    "2. calculates volumes to be added via echo to reach desired input DNA quantity\n",
    "3. produces an Echo-formatted pick list file"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Part 1 of 4, Step 0 of 8: Provide inputs"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "expt_name = \"RKL4982\"",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# One dictionary per study included in the samples on this run.\n",
    "studies_info = [\n",
    "    # EVERY entry in the dictionary must be specifically updated \n",
    "    # *every* time this notebook is run--none of these have defaults!\n",
    "    {\n",
    "    'Project Name': 'Celeste_Adaptation_12986', # PROJECTNAME_QIITAID\n",
    "    'Project Abbreviation': 'ADAPT', # PROJECTNAME\n",
    "    'sample_accession_fp': './test_data/Plate_Maps/sa_file_1.tsv',\n",
    "    'qiita_metadata_fp': './test_data/Plate_Maps/12986_20230314-090655.txt',\n",
    "    'experiment_design_description': 'isolate sequencing',\n",
    "    'HumanFiltering': 'False', \n",
    "    'Email': 'r@gmail.com'\n",
    "    },\n",
    "    {\n",
    "    'Project Name': 'CHILD_15510', # PROJECTNAME_QIITAID\n",
    "    'Project Abbreviation': 'CHILD', # PROJECTNAME\n",
    "    'sample_accession_fp': './test_data/Plate_Maps/sa_file_2.tsv',\n",
    "    'qiita_metadata_fp': './test_data/Plate_Maps/15510_20240503-090339.txt',\n",
    "    'experiment_design_description': 'whole genome sequencing',\n",
    "    'HumanFiltering': 'True',\n",
    "    'Email': 'l@ucsd.edu'\n",
    "    },\n",
    "    {\n",
    "    'Project Name': 'Celeste_Marmoset_14577', # PROJECTNAME_QIITAID\n",
    "    'Project Abbreviation': 'MARMO', # PROJECTNAME\n",
    "    'sample_accession_fp': './test_data/Plate_Maps/sa_file_3.tsv',\n",
    "    'qiita_metadata_fp': './test_data/Plate_Maps/14577_20230711-082202.txt',\n",
    "    'experiment_design_description': 'whole genome sequencing',\n",
    "    'HumanFiltering': 'False',\n",
    "    'Email': 'c@ucsd.edu'\n",
    "    }\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: ask what you put in here when doing replicates\n",
    "compression_layout = [\n",
    "    {\n",
    "        # top left plate\n",
    "        'Plate Position': 1, # as int\n",
    "        'Plate map file': './test_data/Plate_Maps/2022_summer_Celeste_Adaptation_16_plate_map.tsv',\n",
    "        'Project Name': 'Celeste_Adaptation_12986', # PROJECTNAME_QIITAID\n",
    "        'Project Plate': 'Plate_16', # Plate_#\n",
    "        'Plate elution volume': 70\n",
    "    },\n",
    "    {\n",
    "        # top right plate\n",
    "        'Plate Position': 2, # as int\n",
    "        'Plate map file': './test_data/Plate_Maps/2022_summer_Celeste_Adaptation_17_plate_map.tsv',\n",
    "        'Project Name': 'Celeste_Adaptation_12986', # PROJECTNAME_QIITAID\n",
    "        'Project Plate': 'Plate_17', # Plate_#\n",
    "        'Plate elution volume': 70\n",
    "    },\n",
    "    {\n",
    "        # bottom left plate\n",
    "        'Plate Position': 3, # as int\n",
    "        'Plate map file': './test_data/Plate_Maps/2022_summer_Celeste_Adaptation_18_plate_map.tsv',\n",
    "        'Project Name': 'Celeste_Adaptation_12986', # PROJECTNAME_QIITAID\n",
    "        'Project Plate': 'Plate_18', # Plate_#\n",
    "        'Plate elution volume': 70\n",
    "    },\n",
    "    {\n",
    "        # bottom right plate\n",
    "        'Plate Position': 4, # as int\n",
    "        'Plate map file': './test_data/Plate_Maps/CHILD_1000_plate_map.tsv',\n",
    "        'Project Name': 'CHILD_15510', # PROJECTNAME_QIITAID    \n",
    "        'Project Plate': 'Plate_1000',  # Plate_#\n",
    "        'Plate elution volume': 70\n",
    "    },\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# CONSTANTS: Users, DO NOT CHANGE THESE\n",
    "# values without consulting with tech team\n",
    "SHEET_TYPE_VERSIONS = {\n",
    "    STANDARD_METAG_SHEET_TYPE: '101',  # version supporting SampleContext\n",
    "    ABSQUANT_SHEET_TYPE: '11'\n",
    "}\n",
    "\n",
    "BIOINFO_BASE = {\n",
    "    'ForwardAdapter': 'GATCGGAAGAGCACACGTCTGAACTCCAGTCAC',\n",
    "    'ReverseAdapter': 'GATCGGAAGAGCGTCGTGTAGGGAAAGGAGTGT',\n",
    "    'library_construction_protocol': 'Knight Lab Kapa HyperPlus',\n",
    "    # The BarcodesAreRC value is no longer used, but is still checked for\n",
    "    # by the validation while making the sample sheet, so put in a dummy value\n",
    "    'BarcodesAreRC': 'True'\n",
    "}\n",
    "\n",
    "WELL_COL = 'Well'\n",
    "LIB_WELL_COL = 'Library Well'\n",
    "\n",
    "# Mask arrays for even and odd rows and columns\n",
    "EVEN_ROWS = [x for x in range(16) if x % 2 == 0]\n",
    "ODD_ROWS = [x for x in range(16) if x % 2 == 1]\n",
    "EVEN_COLS = [x for x in range(24) if x % 2 == 0]\n",
    "ODD_COLS = [x for x in range(24) if x % 2 == 1]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_studies_attr_list(studies_dict, desired_key):\n",
    "    return [x[desired_key] for x in studies_dict]\n",
    "\n",
    "def pick_expected_separator(fps_list):\n",
    "    sep = \"\\t\"\n",
    "    visible_sep = \"tab\"\n",
    "    \n",
    "    num_fps = len(fps_list)\n",
    "    num_csv = sum([x.endswith('.csv') for x in fps_list])\n",
    "    num_txt = sum([x.endswith('.txt') for x in fps_list])\n",
    "    num_tsv = sum([x.endswith('.tsv') for x in fps_list])\n",
    "    \n",
    "    if num_csv == num_fps:\n",
    "        sep = ','\n",
    "        visible_sep = \"comma\"\n",
    "    elif (num_tsv + num_txt) != num_fps:\n",
    "        warnings.warn(\n",
    "            \"Could not determine separator; defaulting to \" + visible_sep)\n",
    "\n",
    "    return sep, visible_sep"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Part 1 of 4, Step 1 of 8: Read in sample accession files"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# read in the sample accession files\n",
    "sample_accession_fps = get_studies_attr_list(\n",
    "    studies_info, 'sample_accession_fp')\n",
    "sample_acc_sep, sa_sep_name = pick_expected_separator(sample_accession_fps)\n",
    "print(f\"Expected sample accession separator: {sa_sep_name}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sample_accession_df = join_dfs_from_files(\n",
    "    sample_accession_fps, [SAMPLE_NAME_KEY, TUBECODE_KEY], sep=sample_acc_sep)\n",
    "sample_accession_df.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sample_accession_df.head()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Part 1 of 4, Step 2 of 8: Read in the sample info from Qiita"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# read in the qiita metadata files\n",
    "qiita_metadata_fps = get_studies_attr_list(studies_info, 'qiita_metadata_fp')\n",
    "qiita_metadata_sep, qm_sep_name = pick_expected_separator(qiita_metadata_fps)\n",
    "print(f\"Expected qiita metadata separator: {qm_sep_name}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "metadata_df = join_dfs_from_files(\n",
    "    qiita_metadata_fps, [SAMPLE_NAME_KEY, QIITA_STUDY_ID_KEY], \n",
    "    opt_cols_to_extract=['tube_id'], unique_cols=[SAMPLE_NAME_KEY],\n",
    "    sep=qiita_metadata_sep)\n",
    "metadata_df.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "metadata_df.head()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now use the metadata to link the study info into the sample accession dataframe:"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "extended_sample_accession_df = extend_sample_accession_df(\n",
    "    sample_accession_df, studies_info, metadata_df)\n",
    "extended_sample_accession_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Part 1 of 4, Step 3 of 8: Assign the compression layout and add controls"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# copy study info into the compression layout dictionary (so that it doesn't \n",
    "# have to be entered manually in both places)\n",
    "extended_compression_layout = extend_compression_layout_info(\n",
    "    compression_layout, studies_info)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plate_df = compress_plates(extended_compression_layout, \n",
    "                           extended_sample_accession_df, well_col=WELL_COL)\n",
    "plate_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Check for samples with missing names; at this point, we expect all blanks\n",
    "and katharoseq controls WON'T have names."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def check_nan_samples(a_plate_df, blanks_dir=None):\n",
    "    num_remaining_nans = a_plate_df[a_plate_df[PM_SAMPLE_KEY].isna()].shape[0]\n",
    "    print(\"Number of samples with missing names: %d\" % num_remaining_nans)\n",
    "    \n",
    "    if num_remaining_nans > 0 and blanks_dir:\n",
    "        err_msg = f\"\"\"\n",
    "By now, all samples should have names, so **do not continue** before fixing this!\n",
    "\n",
    "\"Unofficial\" blanks are the most likely issue.\n",
    "Determine if the tube codes for the problem samples (shown below) are blanks.\n",
    "If they are, add them to the missing_blanks.csv file in the {blanks_dir} directory.\n",
    "Then re-run from 'Part 1 of 4, Step 3 of 8: Assign the compression layout and add controls'.\"\"\"\n",
    "        print(err_msg)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "check_nan_samples(plate_df)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "blanks_dir = './test_data/BLANKS'\n",
    "# ATTENTION: Does your plate include katharoseq controls?\n",
    "# If *yes*, replace the None below with the path to the directory they are in, such as\n",
    "# katharoseq_dir = './test_data/katharoseq'\n",
    "katharoseq_dir = None\n",
    "\n",
    "plate_df = add_controls(plate_df, blanks_dir, katharoseq_dir)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "After adding controls, check again for samples with missing names; \n",
    "at this point, we expect all blanks and katharoseq controls WILL have names, \n",
    "so if there are any remaining samples without names, \n",
    "stop processing and fix them!"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "check_nan_samples(plate_df, blanks_dir=blanks_dir)\n",
    "plate_df[plate_df[PM_SAMPLE_KEY].isna()]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Part 1 of 4, Step 4 of 8: Validate plate dataframe"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# note that this function does not *need* the extended sample accession df,\n",
    "# but it is easier to use it just to keep things consistent\n",
    "validate_plate_df(plate_df,metadata_df, extended_sample_accession_df, \n",
    "                  blanks_dir, katharoseq_dir)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 of 4, Step 5 of 8: read in DNA concentrations and add to plate map\n",
    "\n",
    "Enter the path to the Pico DNA concentration output. This should be\n",
    " a tab-separated file produced by the MiniPico assay on the condensed, \n",
    " 384-well plate, and should have a format like the below:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "```\n",
    "##BLOCKS= 1\n",
    "Group: Unknowns\n",
    "Sample\tWells\tRFU_Values\tConcentration\tMean_Conc\tSD\tCV\tDilution\tAdjConc\t\n",
    "01\tA1\t528791.000\t2.472\t2.472\t0.000\t0.0\t\t\t\n",
    "02\tC1\t481728.000\t2.282\t2.282\t0.000\t0.0\t\t\t\n",
    "03\tE1\t462964.000\t2.206\t2.206\t0.000\t0.0\t\t\t\n",
    "04\tG1\t556609.000\t2.585\t2.585\t0.000\t0.0\t\t\t\n",
    "05\tI1\t710679.000\t3.207\t3.207\t0.000\t0.0\t\t\t\n",
    "06\tK1\t655693.000\t2.985\t2.985\t0.000\t0.0\t\t\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sample_concs_fp = './test_data/Quant/MiniPico/2022_07_Celeste_Adaptation_16_17_18_21_gDNA_quant.txt'\n",
    "\n",
    "if not os.path.isfile(sample_concs_fp):\n",
    "    print(\"Problem! %s is not a path to a valid file\" % sample_concs_fp)\n",
    "sample_concs = read_pico_csv(sample_concs_fp, plate_reader='SpectraMax_i3x')\n",
    "sample_concs.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plate_df = pd.merge(plate_df, sample_concs, on=WELL_COL)\n",
    "plate_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if requires_dilution(plate_df,threshold=10,tolerance=.10):\n",
    "    plate_df = dilute_gDNA(plate_df,threshold=10)\n",
    "    print('You need to make a 1:10 gDNA dilution plate.')\n",
    "else:\n",
    "    plate_df['extracted_gdna_concentration_ng_ul'] = \\\n",
    "        plate_df['Sample DNA Concentration'].copy()\n",
    "    plate_df['Diluted'] = False\n",
    "    print('Proceed w/out gDNA dilutions')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Visualize plate DNA concentrations and plate map:**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# get DNA concentration information\n",
    "dna_concs = make_2D_array(plate_df, data_col='Sample DNA Concentration', \n",
    "                          well_col=WELL_COL).astype(float)\n",
    "\n",
    "# get information for annotation\n",
    "names = make_2D_array(plate_df, data_col=PM_SAMPLE_KEY, well_col=WELL_COL)\n",
    "\n",
    "plot_plate_vals(dna_concs,\n",
    "                annot_str=names,\n",
    "                color_map='viridis',\n",
    "                annot_fmt='.5s')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make sample replicates\n",
    "\n",
    "Set replicate dictionary, if needed."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Replicate formats:\n",
    "# replicate_dict = {source1_quadrant:destination1_quadrant}\n",
    "# replicate_dict = {source1_quadrant:[destination1_quadrants,destination1_quadrants]}\n",
    "# Replicate example: \n",
    "# replicate_dict = {1:[2,3]}\n",
    "# for no replicates, use:\n",
    "replicate_dict = None\n",
    "\n",
    "# 'Well' differs from 'Library Well' because the former specifies the \n",
    "# gDNA source well while the latter specifies the well (destination well) that \n",
    "# will contain the sequencing library for the sample. These contain the same\n",
    "# info when replicates are not used, but differ when replicates ARE used,\n",
    "# so it is safer to use 'Library Well' in both cases.\n",
    "# (Careful!  well_col is a global variable used throughout rest of notebook)\n",
    "well_col = LIB_WELL_COL"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# initialize new PlateReplication object to manage metadata, conversions, etc.\n",
    "# initialize w/preferred well_col.\n",
    "pr = PlateReplication(well_col)\n",
    "\n",
    "# set overwrite=False to detect any overwriting of source or destination quads \n",
    "# and raise an Error.\n",
    "plate_df = pr.make_replicates(\n",
    "    plate_df, replicates=replicate_dict, overwrite=True)\n",
    "\n",
    "# replicates overlapping sample_wells for other samples should raise warning,\n",
    "# but will be allowed\n",
    "if 'True' in plate_df['contains_replicates'].unique():\n",
    "    plate_df['contains_replicates'] = True\n",
    "    # get DNA concentration information\n",
    "    dna_concs = make_2D_array(plate_df, data_col='Sample DNA Concentration', \n",
    "                              well_col=well_col).astype(float)\n",
    "\n",
    "    # get information for annotation\n",
    "    names = make_2D_array(plate_df, data_col=PM_SAMPLE_KEY, well_col=well_col)\n",
    "\n",
    "    plot_plate_vals(dna_concs,\n",
    "                annot_str=names,\n",
    "                color_map='viridis',\n",
    "                annot_fmt='.6s')\n",
    "else:\n",
    "    plate_df['contains_replicates'] = False\n",
    "    \n",
    "# show whether this plate contains replicates or not\n",
    "f\"Contains replicates: {plate_df['contains_replicates'].unique()}\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### gDNA concentration heatmap, Plate 1"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plot_plate_vals(dna_concs[np.ix_(EVEN_ROWS,EVEN_COLS)],\n",
    "                annot_str= names[np.ix_(EVEN_ROWS,EVEN_COLS)],\n",
    "                color_map='viridis',\n",
    "                annot_fmt='')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### gDNA concentration heatmap, Plate 2"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plot_plate_vals(dna_concs[np.ix_(EVEN_ROWS,ODD_COLS)],\n",
    "                    annot_str= names[np.ix_(EVEN_ROWS,ODD_COLS)],\n",
    "                    color_map='viridis',\n",
    "                    annot_fmt='')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### gDNA concentration heatmap, Plate 3"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plot_plate_vals(dna_concs[np.ix_(ODD_ROWS,EVEN_COLS)],\n",
    "                    annot_str= names[np.ix_(ODD_ROWS,EVEN_COLS)],\n",
    "                    color_map='viridis',\n",
    "                    annot_fmt='')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### gDNA concentration heatmap, Plate 4"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plot_plate_vals(dna_concs[np.ix_(ODD_ROWS,ODD_COLS)],\n",
    "                    annot_str= names[np.ix_(ODD_ROWS,ODD_COLS)],\n",
    "                    color_map='viridis',\n",
    "                    annot_fmt='')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 of 4, Step 6 of 8: calculate normalization volumes and add to plate map\n",
    "\n",
    "This step will calculate volumes for the DNA normalization pick list.\n",
    "\n",
    "Check the desired values for:\n",
    " - **`ng`**: the desired quantity of DNA in normed plate, in ng\n",
    " - **`total_vol`**: the total volume of normalized DNA, in nL\n",
    " - **`min_vol`**: the minimum quantity of sample to add, in nL\n",
    " - **`resolution`**: the resolution of the Echo, in nL (usually 2.5)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "ng = 5\n",
    "total_vol = 3500\n",
    "min_vol = 25\n",
    "resolution = 2.5\n",
    "\n",
    "dna_vols = calculate_norm_vol(\n",
    "    plate_df['Sample DNA Concentration'], ng=ng, min_vol=min_vol, \n",
    "    max_vol=total_vol, resolution=resolution)\n",
    "water_vols = total_vol - dna_vols\n",
    "\n",
    "plate_df['Normalized DNA volume'] = dna_vols\n",
    "plate_df['Normalized water volume'] = water_vols\n",
    "\n",
    "plate_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Part 1 of 4, Step 7 of 8 (optional): Add synDNA spike-in"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Set syndna_pool_number to 1 if syndna is being used; otherwise, leave as None\n",
    "syndna_pool_number = None\n",
    "plate_df = add_syndna(plate_df, \n",
    "                      syndna_pool_number=syndna_pool_number,\n",
    "                      syndna_concentration=2.22)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def is_absquant(a_plate_df):\n",
    "    return bool(np.any(a_plate_df['syndna_pool_number'].unique()))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "f'For this plate, is_absquant = {is_absquant(plate_df)}'\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if is_absquant(plate_df):\n",
    "    syndna_well='A1'\n",
    "    syndna_plate = 'synDNA plate'\n",
    "    syndna_picklist = \\\n",
    "        format_dna_norm_picklist(\n",
    "            np.array(plate_df['synDNA volume']),\n",
    "            np.zeros(plate_df.shape[0]),\n",
    "            np.repeat(syndna_well,plate_df.shape[0]),\n",
    "            dest_wells = np.array(plate_df[well_col]),\n",
    "            sample_names = np.array(plate_df[PM_SAMPLE_KEY]),\n",
    "            sample_plates = np.repeat(syndna_plate,plate_df.shape[0]))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if is_absquant(plate_df):\n",
    "    # Write the picklist as .txt\n",
    "    syndna_picklist_fp = './test_output/Input_Norm/YYYY_MM_DD_Celeste_Adaptation_16-21_matrix_syndna_absquant.txt'\n",
    "\n",
    "    if os.path.isfile(syndna_picklist_fp):\n",
    "        print(\"Warning! This file exists already.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if is_absquant(plate_df):\n",
    "    with open(syndna_picklist_fp, 'w') as f:\n",
    "        f.write(syndna_picklist)\n",
    "\n",
    "    !head {syndna_picklist_fp}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 of 4, Step 8 of 8: Make pick list and write to file\n",
    "\n",
    "Format the Echo-compatible pick list."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "norm_picklist = format_dna_norm_picklist(\n",
    "    np.array(plate_df['Normalized DNA volume']),\n",
    "    np.array(plate_df['Normalized water volume']),\n",
    "    np.array(plate_df['Well']),\n",
    "    dest_wells = np.array(plate_df[well_col]),\n",
    "    sample_names = np.array(plate_df[PM_SAMPLE_KEY]),\n",
    "    sample_plates = np.array(plate_df['Compressed Plate Name']),\n",
    "    dna_concs = np.array(plate_df['Sample DNA Concentration']))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write the picklist as .txt\n",
    "norm_picklist_fp = './test_output/Input_Norm/YYYY_MM_DD_Celeste_Adaptation_16-21_inputnorm.txt'\n",
    "\n",
    "if os.path.isfile(norm_picklist_fp):\n",
    "    print(\"Warning! This file exists already.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with open(norm_picklist_fp, 'w') as f:\n",
    "    f.write(norm_picklist)\n",
    "    \n",
    "!head {norm_picklist_fp}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 (of 4): Workflow for assigning barcodes\n",
    "\n",
    "This portion of the notebook will assign index values and construct an Echo picklist file for adding barcodes. \n",
    "\n",
    "As inputs, it requires:\n",
    "1. A plate_df dataframe (from previous step)\n",
    "2. A tab-delimited index combination file, relating index combinations, i5 and i7 index values, and i5 and i7 index locations\n",
    "\n",
    "The workflow then:\n",
    "1. reads in the index combo list\n",
    "2. assigns indices per sample\n",
    "3. produces an Echo-formatted pick list file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 of 4, Step 1 of 3: Read in index combo list\n",
    "\n",
    "This is a file that contains every possible i5 and i7 barcode combo on a separate line,\n",
    "along with plate and well location information. It should look something like this:\n",
    "\n",
    "```\n",
    "index combo,index combo seq,i5 name,i5 sequence,i5 well,i5 plate,i7 name,i7 sequence,i7 well,i7 plate\n",
    "0,ACCGACAAACGTTACC,iTru5_01_A,ACCGACAA,A1,iTru5_plate,iTru7_101_01,ACGTTACC,A1,iTru7_plate\n",
    "1,AGTGGCAACTGTGTTG,iTru5_01_B,AGTGGCAA,B1,iTru5_plate,iTru7_101_02,CTGTGTTG,A2,iTru7_plate\n",
    "2,CACAGACTTGAGGTGT,iTru5_01_C,CACAGACT,C1,iTru5_plate,iTru7_101_03,TGAGGTGT,A3,iTru7_plate\n",
    "3,CGACACTTGATCCATG,iTru5_01_D,CGACACTT,D1,iTru5_plate,iTru7_101_04,GATCCATG,A4,iTru7_plate\n",
    "4,GACTTGTGGCCTATCA,iTru5_01_E,GACTTGTG,E1,iTru5_plate,iTru7_101_05,GCCTATCA,A5,iTru7_plate\n",
    "5,GTGAGACTAACAACCG,iTru5_01_F,GTGAGACT,F1,iTru5_plate,iTru7_101_06,AACAACCG,A6,iTru7_plate\n",
    "6,GTTCCATGACTCGTTG,iTru5_01_G,GTTCCATG,G1,iTru5_plate,iTru7_101_07,ACTCGTTG,A7,iTru7_plate\n",
    "7,TAGCTGAGCCTATGGT,iTru5_01_H,TAGCTGAG,H1,iTru5_plate,iTru7_101_08,CCTATGGT,A8,iTru7_plate\n",
    "8,CTTCGCAATGTACACC,iTru5_02_A,CTTCGCAA,I1,iTru5_plate,iTru7_101_09,TGTACACC,A9,iTru7_plate\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "index_combo_fp = './test_output/iTru/new_iTru_combos_Dec2017.csv'\n",
    "\n",
    "if not os.path.isfile(index_combo_fp):\n",
    "    print(\"Problem! %s is not a path to a valid file\" % index_combo_fp)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO: check if changing to set dtype = str causes issues\n",
    "#  needs to be cast after reading in the file\n",
    "index_combos = pd.read_csv(index_combo_fp)\n",
    "index_combos.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 of 4, Step 2 of 3: Assign index combo\n",
    "\n",
    "This will pick a set of index combos from the index combo for the number of samples in the `plate_df` DataFrame.\n",
    "\n",
    "Keep track of the barcode combinations used in the lab, and set `starting_combo` equal to the next unused combination.\n",
    "\n",
    "One of way of doing that might be to keep track of the number of plates run, and set `starting_combo` equal to\n",
    "384 * number of plates run + 1."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plate_counter = 144\n",
    "\n",
    "starting_combo = ((plate_counter - 1) % 384) * 384\n",
    "indices = assign_index(len(plate_df[PM_SAMPLE_KEY]), index_combos, \n",
    "                       start_idx=starting_combo).reset_index()\n",
    "\n",
    "plate_df = pd.concat([plate_df, indices], axis=1)\n",
    "plate_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 of 4, Step 3 of 3: Make index pick list and write to file\n",
    "\n",
    "Format the Echo-compatible pick list."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "index_picklist = format_index_picklist(\n",
    "    plate_df[PM_SAMPLE_KEY], plate_df[well_col], indices)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write the picklist as .txt\n",
    "index_picklist_fp = './test_output/Indices/YYYY_MM_DD_Celeste_Adaptation_16-21_indices_matrix.txt'\n",
    "\n",
    "if os.path.isfile(index_picklist_fp):\n",
    "    print(\"Warning! This file exists already.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with open(index_picklist_fp, 'w') as f:\n",
    "    f.write(index_picklist)\n",
    "\n",
    "!head {index_picklist_fp}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 (of 4): Workflow for calculating pooling\n",
    "\n",
    "This portion of the notebook calculates pooling based on fluorescent\n",
    " quantification values and produces visual outputs to interpret and check \n",
    " values. \n",
    "\n",
    "As inputs, this workflow requires:\n",
    "1. A plate map DataFrame (from previous step)\n",
    "2. MiniPico output (tab-delimited text format with columns 'Concentration' and 'Well')\n",
    "\n",
    "The workflow:\n",
    "1. reads in MiniPico output and calculates estimated library concentration\n",
    "2. calculates pooling values and generates an Echo pick list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 of 4, Step 1 of 7: read in MiniPico library concentration\n",
    "Enter correct path to MiniPico file:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "lib_concs_fp = './test_data/Quant/MiniPico/2022_07_Celeste_Adaptation_16_17_18_21_CleanLib_quant.txt'",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "lib_concs = read_pico_csv(lib_concs_fp, plate_reader='SpectraMax_i3x',\n",
    "                          conc_col_name='MiniPico Library DNA Concentration')\n",
    "lib_concs.rename(columns={'Well':well_col},inplace=True)\n",
    "plate_df = pd.merge(plate_df, lib_concs, on=well_col)\n",
    "\n",
    "plate_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 of 4, Step 2 of 7: calculate sample concentration from MiniPico\n",
    "\n",
    "You will want to make sure that 'size' is correct for your average library size."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plate_df['MiniPico Library Concentration'] = \\\n",
    "    compute_pico_concentration(\n",
    "        plate_df['MiniPico Library DNA Concentration'], size=500)\n",
    "plate_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 of 4, Step 3 of 7: visualize MiniPico values\n",
    "\n",
    "This step will present visuals of the results, including:\n",
    "1. Scatter plot of DNA concentrations by Library concentration\n",
    "2. Plate-wise heatmap and histogram showing library concentrations\n",
    "3. per-96-well plate heatmaps and histograms showing library concentrations and sample names\n",
    "4. Plate-wise heatmap showing pooling values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Library concentration by sample DNA concentration:"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "f, (ax1,ax2,ax3) = plt.subplots(nrows=1, ncols=3, figsize=(14, 4))\n",
    "plate_df['Input DNA'] = plate_df['Sample DNA Concentration']*plate_df['Normalized DNA volume']/1000\n",
    "sns.regplot(x=\"Sample DNA Concentration\", y=\"MiniPico Library DNA Concentration\", data=plate_df, ax = ax1)\n",
    "sns.boxplot(x=\"Blank\", y=\"MiniPico Library DNA Concentration\", data=plate_df, ax = ax2)\n",
    "sns.swarmplot(x=\"Blank\", y=\"MiniPico Library DNA Concentration\", data=plate_df, ax = ax2,\n",
    "              size=3,color='black',alpha=0.5)\n",
    "sns.scatterplot( x=\"Input DNA\",y=\"MiniPico Library DNA Concentration\",hue='Sample DNA Concentration',data=plate_df ,ax = ax3)\n",
    "ax3.legend(title='Sample DNA Concentration',loc='center left', bbox_to_anchor=(1, 0.5))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "blanks_gdna_concs = plate_df.loc[plate_df['Blank']==True,'Sample DNA Concentration']\n",
    "samples_gdna_concs = plate_df.loc[plate_df['Blank']==False,'Sample DNA Concentration']\n",
    "mannwhitneyu(samples_gdna_concs, blanks_gdna_concs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "blanks_lib_concs = plate_df.loc[plate_df['Blank']==True,'MiniPico Library Concentration']\n",
    "samples_lib_concs = plate_df.loc[plate_df['Blank']==False,'MiniPico Library Concentration']\n",
    "mannwhitneyu(samples_lib_concs, blanks_lib_concs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Library concentration heatmap, whole plate"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# get concentration and pooling values for plotting\n",
    "concs = make_2D_array(plate_df, data_col=\"MiniPico Library Concentration\", well_col=well_col).astype(float)\n",
    "dna = make_2D_array(plate_df, data_col='Sample DNA Concentration', well_col=well_col).astype(float)\n",
    "\n",
    "# get information for annotation\n",
    "names = make_2D_array(plate_df, data_col=PM_SAMPLE_KEY, well_col=well_col)\n",
    "i5 = make_2D_array(plate_df, data_col='i5 name', well_col=well_col)\n",
    "i7 = make_2D_array(plate_df, data_col='i7 name', well_col=well_col)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "plot_plate_vals(concs, color_map='viridis')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Plate maps for individual constituent plates"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "##### Library concentration heatmap, Plate 1"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plot_plate_vals(concs[np.ix_(EVEN_ROWS,EVEN_COLS)],\n",
    "                    annot_str= names[np.ix_(EVEN_ROWS,EVEN_COLS)],\n",
    "                    color_map='viridis',\n",
    "                    annot_fmt='')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "##### Library concentration heatmap, Plate 2"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plot_plate_vals(concs[np.ix_(EVEN_ROWS,ODD_COLS)],\n",
    "                    annot_str= names[np.ix_(EVEN_ROWS,ODD_COLS)],\n",
    "                    color_map='viridis',\n",
    "                    annot_fmt='')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "##### Library concentration heatmap, Plate 3"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plot_plate_vals(concs[np.ix_(ODD_ROWS,EVEN_COLS)],\n",
    "                    annot_str= names[np.ix_(ODD_ROWS,EVEN_COLS)],\n",
    "                    color_map='viridis',\n",
    "                    annot_fmt='')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "##### Library concentration heatmap, Plate 4"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plot_plate_vals(concs[np.ix_(ODD_ROWS,ODD_COLS)],\n",
    "                    annot_str= names[np.ix_(ODD_ROWS,ODD_COLS)],\n",
    "                    color_map='viridis',\n",
    "                    annot_fmt='')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 of 4, Step 4 of 7: calculate pooling values for MiniPico with autopool\n",
    "\n",
    "This step will calculate the sample pooling, and update the sample data frame with the pool info.\n",
    "There are two automated methods to pool:\n",
    "1. **norm**: This will attempt to generate a normalized pool, automatically inferring the best parameter for pooling.\n",
    "    - ***pool_failures***:\n",
    "        - _high_: will pool failures at the highest pooling volume from optimized pooling.\n",
    "        - _low_: will pool failures at the lowest pooling volume from optimized pooling.\n",
    "\n",
    "2. **evp**: This will pool an even volume per sample.\n",
    "    - ***total_vol***: (Optional, Default: 100µL) The total volume to pool, in uL. Each sample will be pooled at 1/N of that volume, where N is total number of samples in the prep.\n",
    "\n",
    "3. **automate**: (Optional, Default = True) When False, this argument will allow one input parameters for **Legacy** arguments. \n",
    "\n",
    "> **Legacy**\n",
    "> There are legacy parameters to control pooling behaviors when autopool automation (automate=True) returns a poor result. To use these parameters, one must pass automate=False.\n",
    "\n",
    ">   - **min_conc**: (default: 0) This is the minimum concentration for a sample to be considered for pooling.\n",
    "    Set to 0 to pool all samples, regardless of concentration. Increasing this will have the \n",
    "    effect of increasing pool concentration, at the expense of samples dropping out. \n",
    ">   - **floor_conc**: This is the lowest concentration equivalent for which a sample will be \n",
    "    accurately pooled. Samples below this concentration will be pooled to the volume that they \n",
    "    would have been if they were actually that concentration. For example, if `floor_conc=20`, \n",
    "    and a sample at 20 nM pools at 500 nL, a sample at 40 nM will pool at 250 nL but a sample at \n",
    "    10 nM will still pool at 500 nL (rather than 1000). Increasing this value will have the effect \n",
    "    of increasing pool concentration, but decreasing read counts for low-concentration samples. \n",
    ">   - **total_nmol**: This is the total number of molecules to shoot for in the pool. Increasing\n",
    "    this will increase the overall volume of the pool.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Calculate and plot pooling volumes"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "threshold = find_threshold(plate_df['MiniPico Library Concentration'], plate_df['Blank'])\n",
    "plate_df = autopool(plate_df,method='evp',total_vol=190)\n",
    "\n",
    "# visualize\n",
    "print(\"Floor concentration: {}\".format(threshold))\n",
    "vols = make_2D_array(plate_df, data_col='MiniPico Pooled Volume', well_col=well_col).astype(float)\n",
    "conc, vol = estimate_pool_conc_vol(plate_df['MiniPico Pooled Volume'], plate_df['MiniPico Library Concentration'])\n",
    "print(\"Pool concentration: {:.2f}\".format(conc))\n",
    "print(\"Pool volume: {:.2f}\".format(vol))\n",
    "with suppress(np.linalg.LinAlgError):\n",
    "    plot_plate_vals(vols)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "vols = make_2D_array(plate_df, data_col='MiniPico Pooled Volume', well_col=well_col).astype(float)\n",
    "sns.scatterplot(x='MiniPico Library Concentration', y='MiniPico Pooled Volume',data=plate_df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Part 3 of 4, Step 5 of 7: make equal volume pooling pick list and write to a file"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "evp_picklist = format_pooling_echo_pick_list(vols, max_vol_per_well=30000)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write the picklist as .csv\n",
    "evp_picklist_fp = './test_output/Pooling/YYYY_MM_DD_Celeste_Adaptation_evp.csv'\n",
    "\n",
    "if os.path.isfile(evp_picklist_fp):\n",
    "    print(\"Warning! This file exists already.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with open(evp_picklist_fp,'w') as f:\n",
    "    f.write(evp_picklist)\n",
    "\n",
    "!head {evp_picklist_fp}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 of 4, Step 6 of 7: Write plate dataframe to file\n",
    "\n",
    "We want to keep all that useful information together in one place so that\n",
    "it can be easily parsed later. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Write the sample info DataFrame as .txt\n",
    "plate_df_fp = './test_output/QC/YYYY_MM_DD_Celeste_Adaptation_matrix_df.txt'\n",
    "\n",
    "if os.path.isfile(plate_df_fp):\n",
    "    print(\"Warning! This file exists already.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plate_df.to_csv(plate_df_fp, sep='\\t')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 of 4, Step 7 of 7: Make sample sheets and write to files\n",
    "\n",
    "This step takes the pooled sample information and writes Illumina sample \n",
    "sheets that can be given directly to the sequencing center.\n",
    "\n",
    "`# TODO: is this true? I thought Sample Name was NOT bcl2fastq-compatible?`\n",
    "\n",
    "bcl2fastq-compatible names will be in the **`Sample ID`** and **`Sample Name`** columns. The\n",
    "original sample names will be in the **`Description`** column.\n",
    "\n",
    "Modify **`lanes`** to indicate which lanes this pool will be sequenced on.\n",
    "\n",
    "**Project Name** and **Project Plate** values will be placed in the \n",
    "**`Sample_Project`** and **`Sample_Name`** columns, respectively."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Determine the sample sheet type to make\n",
    "expt_type = ABSQUANT_SHEET_TYPE if is_absquant(plate_df) \\\n",
    "    else STANDARD_METAG_SHEET_TYPE\n",
    "expt_type"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`bcl2fastq` requires *only* alphanumeric, hyphens, and underscore characters. \n",
    "We'll replace all non-those characters with underscores and add the \n",
    "bcl2fastq-compatible names to the DataFrame."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set bcl2fastq-compatible sample ids\n",
    "plate_df['sample sheet Sample_ID'] = \\\n",
    "    plate_df[PM_SAMPLE_KEY].map(bcl_scrub_name)\n",
    "plate_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Store the final pre-sample-sheet version of the plate dataframe\n",
    "plate_df_fp = './test_output/QC/YYYY_MM_DD_Celeste_Adaptation_matrix_df_complete.txt'\n",
    "plate_df.to_csv(plate_df_fp, sep='\\t')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Extend the metadata dictionary with additional information\n",
    "metadata_dict_w_sample_context = make_sections_dict(\n",
    "    plate_df, studies_info, expt_name,\n",
    "    expt_type, SHEET_TYPE_VERSIONS[expt_type], BIOINFO_BASE)\n",
    "metadata_dict_w_sample_context"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**`sequencer`** is important for making sure the i5 index is in the correct \n",
    "orientation for demultiplexing. `NovaSeq6000`, `HiSeq4000`, `HiSeq3000`, \n",
    "`NextSeq`, `MiniSeq`, and `iSeq` all require reverse-complemented i5 \n",
    "index sequences. If you enter one of these exact strings in for `sequencer`, \n",
    "it will revcomp the i5 sequence for you.\n",
    "\n",
    "`HiSeq2500`, `MiSeq`, `NovaSeqX`, and `NovaSeqXPlus` will NOT revcomp the \n",
    "i5 sequence. "
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Make and save the iSeq sample sheet"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lanes = [1]\n",
    "iseq_sheet = make_sample_sheet(metadata_dict_w_sample_context, \n",
    "                               plate_df, 'iSeq', lanes)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write the iseq samplesheet as .csv\n",
    "iseq_sample_sheet_fp = './test_output/SampleSheets/YYYY_MM_DD_Celeste_Adaptation_12986_16_17_18_21_matrix_samplesheet_iseq.csv'\n",
    "\n",
    "if os.path.isfile(iseq_sample_sheet_fp):\n",
    "    print(\"Warning! This file exists already.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(iseq_sample_sheet_fp,'w') as f:\n",
    "    iseq_sheet.write(f)\n",
    "    \n",
    "!head -n 30 {iseq_sample_sheet_fp}\n",
    "!echo ...\n",
    "!tail -n 15 {iseq_sample_sheet_fp}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Make and save NovaSeq6000 sample sheet"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lanes = [1]\n",
    "novaseq_sheet = make_sample_sheet(metadata_dict_w_sample_context, \n",
    "                                  plate_df, 'NovaSeq6000', lanes)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Write the novaseq samplesheet as .csv\n",
    "novaseq_sample_sheet_fp = './test_output/SampleSheets/YYYY_MM_DD_Celeste_Adaptation_12986_16_17_18_21_matrix_samplesheet_novaseq.csv'\n",
    "\n",
    "if os.path.isfile(novaseq_sample_sheet_fp):\n",
    "    print(\"Warning! This file exists already.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(novaseq_sample_sheet_fp,'w') as f:\n",
    "    novaseq_sheet.write(f)\n",
    "    \n",
    "!head -n 30 {novaseq_sample_sheet_fp}\n",
    "!echo ...\n",
    "!tail -n 15 {novaseq_sample_sheet_fp}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": "## Part 4 (of 4): Workflow for Read Distribution Summary and Pool Normalization"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 4 of 4, Step 1 of 4: import and merge per_sample read distributions\n",
    "\n",
    "Import tsv file(s) with read_counts from per_sample_fastq files and merge with growing plate_df\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Enter paths to read counts file(s)\n",
    "read_counts_fps = [\n",
    "    './test_data/Demux/YYYY_MM_DD_Celeste_Adaptation_raw_counts.tsv',\n",
    "    './test_data/Demux/YYYY_MM_DD_Celeste_Marmoset_raw_counts.tsv',\n",
    "    './test_data/Demux/YYYY_MM_DD_Child_raw_counts.tsv',\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Import reads counts from file to dataframes\n",
    "CATEGORY_KEY = 'Category'\n",
    "UNIQUE_READS_KEY = 'Unique Reads'\n",
    "DUPLICATE_READS_KEY = 'Duplicate Reads'\n",
    "read_counts_df = join_dfs_from_files(\n",
    "    read_counts_fps, [CATEGORY_KEY, UNIQUE_READS_KEY, DUPLICATE_READS_KEY],\n",
    "    unique_cols=[CATEGORY_KEY], \n",
    "    dtype={CATEGORY_KEY: str, UNIQUE_READS_KEY: int, DUPLICATE_READS_KEY: int})\n",
    "    \n",
    "trimmed_reads_mask = read_counts_df[CATEGORY_KEY].str.contains('trimmed')\n",
    "raw_read_counts_df = read_counts_df.loc[~trimmed_reads_mask].copy()\n",
    "filtered_read_counts_df = read_counts_df.loc[trimmed_reads_mask].copy()\n",
    "\n",
    "##Can also import counts from Qiita per_sample_FASTQ summaries.  \n",
    "# per_sample_fastq_counts_df = pd.read_csv('./test_data/Demux/YYYY_MM_DD_Celeste_Adaptation_16_17_18_21_per_sample_fastq.tsv',\n",
    "#                                          sep='\\t')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Merge read_counts_df with plate_df \n",
    "plate_df_w_reads = merge_read_counts(\n",
    "    plate_df, raw_read_counts_df, \n",
    "    reads_column_name='Raw Reads')\n",
    "plate_df_w_reads = merge_read_counts(\n",
    "    plate_df_w_reads, filtered_read_counts_df, \n",
    "    reads_column_name='Filtered Reads')\n",
    "\n",
    "# plate_df_w_reads = merge_read_counts(\n",
    "#    plate_df_w_reads, per_sample_fastq_counts_fp,\n",
    "#    reads_column_name='Qiita Reads')\n",
    "\n",
    "plate_df_w_reads.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "reads_column = 'Raw Reads'\n",
    "\n",
    "f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2, figsize=(8, 8))\n",
    "# evenness plot\n",
    "rmax = int(round(plate_df_w_reads[reads_column].max(),-2))\n",
    "survival_df = pd.concat([read_survival(plate_df_w_reads.loc[plate_df_w_reads['Blank'] == True,\n",
    "                                                            reads_column], label='Blanks',rmax=rmax),\n",
    "                         read_survival(plate_df_w_reads.loc[plate_df_w_reads['Blank'] == False,\n",
    "                                                            reads_column], label='Samples',rmax=rmax)])\n",
    "\n",
    "ax3.set_xlabel(reads_column)\n",
    "ax3.set_ylabel('Samples')\n",
    "survival_df.plot(color = ['coral','steelblue'],ax=ax1)\n",
    "ax1.set_xlabel(reads_column)\n",
    "ax1.set_ylabel('Samples')\n",
    "\n",
    "##Histogram\n",
    "sns.histplot(plate_df_w_reads[reads_column],ax=ax3)\n",
    "\n",
    "##Regressopm\n",
    "sns.regplot(x=\"MiniPico Library DNA Concentration\", y=reads_column, data=plate_df_w_reads, ax = ax2)\n",
    "\n",
    "#Boxplot\n",
    "sns.boxplot(x=\"Blank\", y=reads_column, data=plate_df_w_reads, ax = ax4)\n",
    "sns.stripplot(x=\"Blank\", y=reads_column, data=plate_df_w_reads, ax = ax4,\n",
    "              size=3,color='black',alpha=0.5)\n",
    "\n",
    "\n",
    "plt.tight_layout()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Part 4 of 4, Step 2 of 4: Calculate iSeqnorm pooling volumes"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "plate_df_normalized = calculate_iseqnorm_pooling_volumes(plate_df_w_reads,dynamic_range=5, normalization_column='Raw Reads')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# visualize\n",
    "vols = make_2D_array(plate_df_normalized, data_col='iSeq normpool volume', well_col=well_col).astype(float)\n",
    "conc, vol = estimate_pool_conc_vol(plate_df_normalized['iSeq normpool volume'], plate_df_normalized['MiniPico Library Concentration'])\n",
    "print(\"Pool concentration: {:.2f}\".format(conc))\n",
    "print(\"Pool volume: {:.2f}\".format(vol))\n",
    "with suppress(np.linalg.LinAlgError):\n",
    "    plot_plate_vals(vols)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Part 4 of 4, Step 3 of 4: Estimate read depth"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Plots estimate of read depth proportion, and returns a df with estimates. \n",
    "plate_df_normalized_with_estimates = estimate_read_depth(plate_df_normalized)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Part 4 of 4, Step 4 of 4: Make pooling picklist and write to a file"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "iseqnormed_picklist = format_pooling_echo_pick_list(vols, max_vol_per_well=30000)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write the picklist as .csv\n",
    "iseqnormed_picklist_fp = './test_output/Pooling/YYYY_MM_DD_Celeste_Adaptation_16_17_18_21_iSeqnormpool.csv'\n",
    "\n",
    "if os.path.isfile(iseqnormed_picklist_fp):\n",
    "    print(\"Warning! This file exists already.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "with open(iseqnormed_picklist_fp,'w') as f:\n",
    "    f.write(iseqnormed_picklist)\n",
    "\n",
    "!head {iseqnormed_picklist_fp}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "473px",
    "width": "381px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "740px",
    "left": "0px",
    "right": "1407.6666259765625px",
    "top": "112px",
    "width": "211.705px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
